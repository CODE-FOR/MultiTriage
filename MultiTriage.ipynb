{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2785aa0-c530-4b45-93f1-d372738a0081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Declaration\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from numpy import asarray\n",
    "from numpy import ones\n",
    "from numpy import zeros\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.python.keras.backend as K\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import backend\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import (\n",
    "    Dense,\n",
    "    Input,\n",
    "    LSTM,\n",
    "    SimpleRNN,\n",
    "    Embedding,\n",
    "    Dropout,\n",
    "    SpatialDropout1D,\n",
    "    Activation,\n",
    "    Conv1D,\n",
    "    GRU,\n",
    "    Reshape,\n",
    ")\n",
    "from keras.layers import (\n",
    "    Conv1D,\n",
    "    Bidirectional,\n",
    "    GlobalMaxPool1D,\n",
    "    MaxPooling1D,\n",
    "    BatchNormalization,\n",
    "    Add,\n",
    "    Flatten,\n",
    ")\n",
    "from keras.layers import (\n",
    "    GlobalMaxPooling1D,\n",
    "    GlobalAveragePooling1D,\n",
    "    concatenate,\n",
    "    SpatialDropout1D,\n",
    ")\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "from tensorflow.python.distribute import distribution_strategy_context\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import sparse_tensor\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import check_ops\n",
    "from tensorflow.python.ops import confusion_matrix\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn\n",
    "from tensorflow.python.ops import sets\n",
    "from tensorflow.python.ops import sparse_ops\n",
    "from tensorflow.python.ops import state_ops\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.python.ops import weights_broadcast_ops\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util.deprecation import deprecated\n",
    "from tensorflow.python.util.tf_export import tf_export\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as nafc\n",
    "import transformers\n",
    "from nlpaug.util import Action\n",
    "\n",
    "from datetime import datetime\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab964f40-3b66-4524-80a0-a091b656d28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Oversample Record with Data Augmentation Group by Developer and Bug Type\n",
    "def CreateOversamplingWithDataAugmentation(data, splitno):\n",
    "    # /*Get Contextual Word Embedding Model*/\n",
    "    aug = naw.ContextualWordEmbsAug(model_path=\"bert-base-uncased\", action=\"substitute\")\n",
    "    # /*Get Developer Bug Count/\n",
    "    dfcountbybug = (\n",
    "        data.groupby([\"Name\", \"FixedByID\"], as_index=True)[\"FixedByID\"]\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "    )\n",
    "    dfcountbybug.to_csv(\"list.csv\")\n",
    "    # /*Get Minority Class List*/\n",
    "    majoritycount = dfcountbybug[\n",
    "        (dfcountbybug[\"FixedByID\"] != \"unknown\") & (dfcountbybug[\"Name\"] != \"unknown\")\n",
    "    ][\"count\"].max()\n",
    "    # /*Get Majority Class Count*/\n",
    "    minoritylist = dfcountbybug[(dfcountbybug[\"count\"] != dfcountbybug[\"count\"].max())]\n",
    "    print(majoritycount, len(minoritylist), majoritycount * len(minoritylist))\n",
    "    estimatetotalnoofdataaugrecord = majoritycount * len(minoritylist)\n",
    "    maxnoofaug = majoritycount\n",
    "    ## Data Aug Record Count Validation, If over threshold reduce the majaritycount\n",
    "    if estimatetotalnoofdataaugrecord > DataAugThreshold:\n",
    "        print(\"Overtheshold--\")\n",
    "        maxnoofaug = int(\n",
    "            (DataAugThreshold / estimatetotalnoofdataaugrecord) * majoritycount\n",
    "        )\n",
    "\n",
    "    # /*Loop through Minor Class Group*/\n",
    "    for ind in minoritylist.index:\n",
    "        # print(minoritylist['FixedByID'][ind], minoritylist['count'][ind])\n",
    "        developer = minoritylist[\"FixedByID\"][ind]\n",
    "        bugtype = minoritylist[\"Name\"][ind]\n",
    "        minoritycount = minoritylist[\"count\"][ind]\n",
    "        data1 = data[(data[\"FixedByID\"] == developer) & (data[\"Name\"] == bugtype)]\n",
    "        # print(len(data1), developer,bugtype)\n",
    "        # print('minoritycount  --->',minoritycount, 'majoritycount--->',majoritycount, 'index --->', ind , 'out of ', len(minoritylist.index))\n",
    "        # Create Sample Data until minority class count Match up with majority class count\n",
    "        while minoritycount < maxnoofaug:\n",
    "            # majoritycount:\n",
    "            samplerow = data1.sample()\n",
    "            oldbugdescription = (\n",
    "                str(samplerow[\"Title_Description\"].values).strip(\"[]\").replace(\"'\", \"\")\n",
    "            )\n",
    "            if oldbugdescription:\n",
    "                first100words_aug = str(\n",
    "                    aug.augment(\" \".join(oldbugdescription.split()[:100]))\n",
    "                )\n",
    "                remainingwords = \" \".join(oldbugdescription.split()[100:])\n",
    "                newbugdescription = first100words_aug + remainingwords\n",
    "                new_row = {\n",
    "                    \"RepoID\": str(samplerow[\"RepoID\"].values)\n",
    "                    .strip(\"[]\")\n",
    "                    .replace(\"'\", \"\"),\n",
    "                    #'PullRequestID' : str(samplerow['PullRequestID'].values).strip('[]').replace(\"'\",\"\"),\n",
    "                    \"IssueID\": str(samplerow[\"IssueID\"].values)\n",
    "                    .strip(\"[]\")\n",
    "                    .replace(\"'\", \"\"),\n",
    "                    \"Title_Description\": newbugdescription,  # /*Data Augmentation : Title_Desciption*/\n",
    "                    \"AST\": str(samplerow[\"AST\"].values).strip(\"[]\").replace(\"'\", \"\"),\n",
    "                    \"FixedByID\": str(samplerow[\"FixedByID\"].values)\n",
    "                    .strip(\"[]\")\n",
    "                    .replace(\"'\", \"\"),\n",
    "                    \"Name\": str(samplerow[\"Name\"].values).strip(\"[]\").replace(\"'\", \"\"),\n",
    "                    \"CreatedDate\": str(samplerow[\"CreatedDate\"].values)\n",
    "                    .strip(\"[]\")\n",
    "                    .replace(\"'\", \"\"),\n",
    "                }\n",
    "                data = data.append(new_row, ignore_index=True)\n",
    "                minoritycount = minoritycount + 1\n",
    "            gc.collect()\n",
    "    trainfilename = DataFileName + \"trainaugdata\" + splitno + FileType\n",
    "    data.to_csv(trainfilename)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bc5f344-a1b2-4aea-85a2-2b37cd79415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_f1_macro(y_true, y_pred):\n",
    "    \"\"\"Computes 3 different f1 scores, micro macro\n",
    "    weighted.\n",
    "    micro: f1 score accross the classes, as 1\n",
    "    macro: mean of f1 scores per class\n",
    "    weighted: weighted average of f1 scores per class,\n",
    "            weighted from the support of each class\n",
    "    Args:\n",
    "        y_true (Tensor): labels, with shape (batch, num_classes)\n",
    "        y_pred (Tensor): model's predictions, same shape as y_true\n",
    "    Returns:\n",
    "        tuple(Tensor): (micro, macro, weighted)\n",
    "                    tuple of the computed f1 scores\n",
    "    \"\"\"\n",
    "\n",
    "    f1s = [0, 0, 0]\n",
    "\n",
    "    y_true = tf.cast(y_true, tf.float64)\n",
    "    y_pred = tf.cast(y_pred, tf.float64)\n",
    "\n",
    "    for i, axis in enumerate([None, 0]):\n",
    "        TP = tf.math.count_nonzero(y_pred * y_true, axis=axis)\n",
    "        FP = tf.math.count_nonzero(y_pred * (y_true - 1), axis=axis)\n",
    "        FN = tf.math.count_nonzero((y_pred - 1) * y_true, axis=axis)\n",
    "\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "        f1s[i] = tf.reduce_mean(f1)\n",
    "\n",
    "    weights = tf.reduce_sum(y_true, axis=0)\n",
    "    weights /= tf.reduce_sum(weights)\n",
    "\n",
    "    f1s[2] = tf.reduce_sum(f1 * weights)\n",
    "\n",
    "    micro, macro, weighted = f1s\n",
    "    return macro\n",
    "\n",
    "\n",
    "def hammingloss(y_true, y_pred, threshold=0.5, mode=\"multilabel\"):\n",
    "    \"\"\"Computes hamming loss.\n",
    "    Hamming loss is the fraction of wrong labels to the total number\n",
    "    of labels.\n",
    "    In multi-class classification, hamming loss is calculated as the\n",
    "    hamming distance between `actual` and `predictions`.\n",
    "    In multi-label classification, hamming loss penalizes only the\n",
    "    individual labels.\n",
    "    Args:\n",
    "        y_true: actual target value\n",
    "        y_pred: predicted target value\n",
    "        threshold: Elements of `y_pred` greater than threshold are\n",
    "            converted to be 1, and the rest 0. If threshold is\n",
    "            None, the argmax is converted to 1, and the rest 0.\n",
    "        mode: multi-class or multi-label\n",
    "    Returns:\n",
    "        hamming loss: float\n",
    "    Usage:\n",
    "    ```python\n",
    "    # multi-class hamming loss\n",
    "    hl = HammingLoss(mode='multiclass', threshold=0.6)\n",
    "    actuals = tf.constant([[1, 0, 0, 0],[0, 0, 1, 0],\n",
    "                       [0, 0, 0, 1],[0, 1, 0, 0]],\n",
    "                      dtype=tf.float32)\n",
    "    predictions = tf.constant([[0.8, 0.1, 0.1, 0],\n",
    "                               [0.2, 0, 0.8, 0],\n",
    "                               [0.05, 0.05, 0.1, 0.8],\n",
    "                               [1, 0, 0, 0]],\n",
    "                          dtype=tf.float32)\n",
    "    hl.update_state(actuals, predictions)\n",
    "    print('Hamming loss: ', hl.result().numpy()) # 0.25\n",
    "    # multi-label hamming loss\n",
    "    hl = HammingLoss(mode='multilabel', threshold=0.8)\n",
    "    actuals = tf.constant([[1, 0, 1, 0],[0, 1, 0, 1],\n",
    "                       [0, 0, 0,1]], dtype=tf.int32)\n",
    "    predictions = tf.constant([[0.82, 0.5, 0.90, 0],\n",
    "                               [0, 1, 0.4, 0.98],\n",
    "                               [0.89, 0.79, 0, 0.3]],\n",
    "                               dtype=tf.float32)\n",
    "    hl.update_state(actuals, predictions)\n",
    "    print('Hamming loss: ', hl.result().numpy()) # 0.16666667\n",
    "    ```\n",
    "    \"\"\"\n",
    "    if mode not in [\"multiclass\", \"multilabel\"]:\n",
    "        raise TypeError(\"mode must be either multiclass or multilabel]\")\n",
    "\n",
    "    if threshold is None:\n",
    "        threshold = tf.reduce_max(y_pred, axis=-1, keepdims=True)\n",
    "        # make sure [0, 0, 0] doesn't become [1, 1, 1]\n",
    "        # Use abs(x) > eps, instead of x != 0 to check for zero\n",
    "        y_pred = tf.logical_and(y_pred >= threshold, tf.abs(y_pred) > 1e-12)\n",
    "    else:\n",
    "        y_pred = y_pred > threshold\n",
    "\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    y_pred = tf.cast(y_pred, tf.int32)\n",
    "\n",
    "    if mode == \"multiclass\":\n",
    "        nonzero = tf.cast(tf.math.count_nonzero(y_true * y_pred, axis=-1), tf.float32)\n",
    "        return 1.0 - nonzero\n",
    "\n",
    "    else:\n",
    "        nonzero = tf.cast(tf.math.count_nonzero(y_true - y_pred, axis=-1), tf.float32)\n",
    "        return nonzero / y_true.get_shape()[-1]\n",
    "\n",
    "\n",
    "def c_precision(y_true, y_pred):\n",
    "    \"\"\"Calculates the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    true_positives = tf.cast(K.sum(K.round(K.clip(y_true * y_pred, 0, 1))), tf.float32)\n",
    "    predicted_positives = tf.cast(K.sum(K.round(K.clip(y_pred, 0, 1))), tf.float32)\n",
    "    precision = tf.cast(\n",
    "        true_positives / (predicted_positives + K.epsilon()), tf.float32\n",
    "    )\n",
    "    return precision\n",
    "\n",
    "\n",
    "def c_recall(y_true, y_pred):\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    \"\"\"Calculates the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = tf.cast(K.sum(K.round(K.clip(y_true * y_pred, 0, 1))), tf.float32)\n",
    "    possible_positives = tf.cast(K.sum(K.round(K.clip(y_true, 0, 1))), tf.float32)\n",
    "    recall = tf.cast(true_positives / (possible_positives + K.epsilon()), tf.float32)\n",
    "    return recall\n",
    "\n",
    "\n",
    "def c_fbeta(y_true, y_pred, beta=2):\n",
    "    y_pred = backend.clip(y_pred, 0, 1)\n",
    "    # calculate elements\n",
    "    tp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "    fp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
    "    fn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
    "    # calculate precision\n",
    "    p = tp / (tp + fp + backend.epsilon())\n",
    "    # calculate recall\n",
    "    r = tp / (tp + fn + backend.epsilon())\n",
    "    # calculate fbeta, averaged across each class\n",
    "    bb = beta**2\n",
    "    fbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n",
    "    return fbeta_score\n",
    "\n",
    "\n",
    "def c_auroc(y_true, y_pred):\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    auc = tf.cast(tf.metrics.auc(y_true, y_pred)[1], tf.float32)\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2260bcce-91bc-467b-b4d8-91f37f3437e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "def Cleandata(data):\n",
    "    porter = PorterStemmer()\n",
    "    for ind in data.index:\n",
    "        bug_description = data[\"Title_Description\"][ind]\n",
    "        bug_description = re.sub(\"[^a-zA-Z]\", \" \", str(bug_description))\n",
    "        if bug_description and bug_description.strip():\n",
    "            tk_bug_description = word_tokenize(bug_description)\n",
    "            tokens_without_sw = (\n",
    "                str(\n",
    "                    [\n",
    "                        word\n",
    "                        for word in tk_bug_description\n",
    "                        if not word in stopwords.words(\"english\")\n",
    "                    ]\n",
    "                )\n",
    "                .strip(\"[]\")\n",
    "                .replace(\"'\", \"\")\n",
    "                .replace(\",\", \"\")\n",
    "                .replace(\"\\\\\", \"\")\n",
    "            )\n",
    "            tokens_without_sl = \" \".join(\n",
    "                [w for w in tokens_without_sw.split() if len(w) > 1]\n",
    "            )  # token without single letter\n",
    "            tokens_with_stemm = \" \".join(\n",
    "                [porter.stem(w) for w in tokens_without_sl.split()]\n",
    "            )  # token with stemm\n",
    "            # data['Title_Description'][ind] = tokens_with_stemm\n",
    "            data.loc[ind, \"Title_Description\"] = tokens_with_stemm\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_tag_mapping(mapping_csv, tagname):\n",
    "    print(\"tagname\", tagname)\n",
    "    # create a set of all known tags\n",
    "    labels = set()\n",
    "    IssueType_Tags = []\n",
    "    for i in range(len(mapping_csv)):\n",
    "        # convert spaced separated tags into an array of tags\n",
    "        tags = mapping_csv[i].split(\"| \")\n",
    "        # add tags to the set of known labels\n",
    "        labels.update(tags)\n",
    "\n",
    "    # convert set of labels to a list to list\n",
    "    labels = list(labels)\n",
    "    # order set alphabetically\n",
    "    labels.sort()\n",
    "    # dict that maps labels to integers, and the reverse\n",
    "    labels_map = {labels[i]: i for i in range(len(labels))}\n",
    "    inv_labels_map = {i: labels[i] for i in range(len(labels))}\n",
    "\n",
    "    for i in range(len(mapping_csv)):\n",
    "        # Create One Hot Encoding For Issue Type\n",
    "        IssueType_Tag = one_hot_encode(mapping_csv[i].split(\"| \"), labels_map)\n",
    "        IssueType_Tags.append(IssueType_Tag)\n",
    "\n",
    "    result = IssueType_Tags\n",
    "    return labels_map, inv_labels_map, result\n",
    "\n",
    "\n",
    "#########SplitTrainTestData################\n",
    "def SplitTrainTestData(data):\n",
    "    x_train_context = []\n",
    "    x_train_AST = []\n",
    "    dev_y_train = []\n",
    "    btype_y_train = []\n",
    "    x_test_context = []\n",
    "    x_test_AST = []\n",
    "    dev_y_test = []\n",
    "    btype_y_test = []\n",
    "    tk_context = []\n",
    "\n",
    "    dev_y = list(data[\"FixedByID\"].astype(str))  # Developer List\n",
    "    btype_y = list(data[\"Name\"].astype(str))  # Bug Type List\n",
    "    data.Title_Description = data.Title_Description.astype(str)\n",
    "    x_context = list(data[\"Title_Description\"])\n",
    "    data.AST = data.AST.astype(str)\n",
    "    x_AST = list(data[\"AST\"])\n",
    "\n",
    "    # 80% / 20% train / test split:\n",
    "    train_size = int(len(x_context) * 0.8)\n",
    "\n",
    "    x_train_context = x_context[:train_size]\n",
    "    x_train_AST = x_AST[:train_size]\n",
    "    dev_y_train = dev_y[:train_size]\n",
    "    btype_y_train = btype_y[:train_size]\n",
    "\n",
    "    x_test_context = x_context[train_size:]\n",
    "    x_test_AST = x_AST[train_size:]\n",
    "    dev_y_test = dev_y[train_size:]\n",
    "    btype_y_test = btype_y[train_size:]\n",
    "\n",
    "    # /*******************Label Encoder***********************/\n",
    "    ### Developer Encoder\n",
    "    combineddata_dev = dev_y_train + dev_y_test\n",
    "    dev_labels_map, dev_inv_labels_map, combineddata_dev_enc = create_tag_mapping(\n",
    "        combineddata_dev, \"FixedByID\"\n",
    "    )\n",
    "    dev_y_train = combineddata_dev_enc[: len(dev_y_train)]\n",
    "    dev_y_test = combineddata_dev_enc[len(dev_y_train) :]\n",
    "    print(\n",
    "        \"Developer\",\n",
    "        \"Training: \",\n",
    "        len(dev_y_train),\n",
    "        \"Testing :\",\n",
    "        len(dev_y_test),\n",
    "        \"Combined DEV + TEST\",\n",
    "        len(combineddata_dev_enc),\n",
    "    )\n",
    "\n",
    "    ### BugType Encoder\n",
    "    combineddata_bugtype = btype_y_train + btype_y_test\n",
    "    (\n",
    "        btype_labels_map,\n",
    "        btype_inv_labels_map,\n",
    "        combineddata_bugtype_enc,\n",
    "    ) = create_tag_mapping(combineddata_bugtype, \"Name\")\n",
    "    btype_y_train = combineddata_bugtype_enc[: len(btype_y_train)]\n",
    "    btype_y_test = combineddata_bugtype_enc[len(btype_y_train) :]\n",
    "    print(\n",
    "        \"Bug Type\",\n",
    "        \"Training: \",\n",
    "        len(btype_y_train),\n",
    "        \"Testing :\",\n",
    "        len(btype_y_test),\n",
    "        \"Combined DEV + TEST\",\n",
    "        len(combineddata_bugtype_enc),\n",
    "    )\n",
    "\n",
    "    # Tokenizer\n",
    "    x_train_context = [str(row).lower() for row in x_train_context]\n",
    "    x_test_context = [str(row).lower() for row in x_test_context]\n",
    "    combineddata = x_train_context + x_test_context\n",
    "    tk_context = Tokenizer(\n",
    "        num_words=None,\n",
    "        char_level=None,\n",
    "        oov_token=\"Unknown\",\n",
    "        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\d+',\n",
    "    )\n",
    "    tk_context.fit_on_texts(combineddata)\n",
    "\n",
    "    return (\n",
    "        tk_context,\n",
    "        x_train_context,\n",
    "        x_train_AST,\n",
    "        dev_y_train,\n",
    "        btype_y_train,\n",
    "        x_test_context,\n",
    "        x_test_AST,\n",
    "        dev_y_test,\n",
    "        btype_y_test,\n",
    "        dev_labels_map,\n",
    "        dev_inv_labels_map,\n",
    "        btype_labels_map,\n",
    "        btype_inv_labels_map,\n",
    "    )\n",
    "\n",
    "\n",
    "def SplitTrainTestDataForMultiLabelWithDataAugmentation(data, splitno):\n",
    "    x_train_context = []\n",
    "    x_train_AST = []\n",
    "    dev_y_train = []\n",
    "    btype_y_train = []\n",
    "    x_test_context = []\n",
    "    x_test_AST = []\n",
    "    dev_y_test = []\n",
    "    btype_y_test = []\n",
    "    tk_context = []\n",
    "\n",
    "    if LoadDataAugFromFile == True:\n",
    "        traindata = pd.read_csv(\n",
    "            DataFilePath\n",
    "            + \"DataAugmentation/\"\n",
    "            + DataFileName\n",
    "            + \"trainaugdata\"\n",
    "            + splitno\n",
    "            + FileType,\n",
    "            error_bad_lines=False,\n",
    "            index_col=False,\n",
    "            dtype=\"unicode\",\n",
    "            encoding=\"latin-1\",\n",
    "            low_memory=False,\n",
    "        ).sample(frac=1)\n",
    "        traindata = traindata.rename(columns={\"ï»¿RepoID\": \"RepoID\"}, inplace=False)\n",
    "\n",
    "        testdata = pd.read_csv(\n",
    "            DataFilePath\n",
    "            + \"DataAugmentation/\"\n",
    "            + DataFileName\n",
    "            + \"testdata\"\n",
    "            + splitno\n",
    "            + FileType,\n",
    "            error_bad_lines=False,\n",
    "            index_col=False,\n",
    "            dtype=\"unicode\",\n",
    "            encoding=\"latin-1\",\n",
    "            low_memory=False,\n",
    "        ).sample(frac=1)\n",
    "        testdata = testdata.rename(columns={\"ï»¿RepoID\": \"RepoID\"}, inplace=False)\n",
    "        testdata = RemoveTestRecordIfNotExistInTrainData(traindata, testdata)\n",
    "    else:\n",
    "        # 80% / 20% train / test split:\n",
    "        train_size = int(len(data) * 0.8)\n",
    "        traindata = data[:train_size]\n",
    "        testdata = data[train_size:]\n",
    "        # Save Data to TestFile\n",
    "        testfilename = DataFileName + \"testdata\" + splitno + FileType\n",
    "        testdata.to_csv(testfilename)\n",
    "        # /*Create Sample Data*/\n",
    "        traindata = CreateOversamplingWithDataAugmentation(traindata, splitno)\n",
    "\n",
    "    # /*****************Train Data***********************/\n",
    "    train_dev_y = list(traindata[\"FixedByID\"])  # Developer List\n",
    "    train_btype_y = list(traindata[\"Name\"])  # Bug Type List\n",
    "    train_x_context = list(traindata[\"Title_Description\"])\n",
    "    traindata.AST = traindata.AST.astype(str)\n",
    "    train_x_AST = list(traindata[\"AST\"])\n",
    "\n",
    "    x_train_context = train_x_context\n",
    "    x_train_AST = train_x_AST\n",
    "\n",
    "    # /****************Test Data************************/\n",
    "    x_test_context = list(testdata[\"Title_Description\"])\n",
    "    x_test_AST = list(testdata[\"Title_Description\"])\n",
    "    test_dev_y = list(testdata[\"FixedByID\"])  # Developer List\n",
    "    test_btype_y = list(testdata[\"Name\"])  # Bug Type List\n",
    "\n",
    "    # /*******************Label Encoder***********************/\n",
    "    ### Developer Encoder\n",
    "    combineddata_dev = train_dev_y + test_dev_y\n",
    "    dev_labels_map, dev_inv_labels_map, combineddata_dev_enc = create_tag_mapping(\n",
    "        combineddata_dev, \"FixedByID\"\n",
    "    )\n",
    "    dev_y_train = combineddata_dev_enc[: len(train_dev_y)]\n",
    "    dev_y_test = combineddata_dev_enc[len(train_dev_y) :]\n",
    "    print(\n",
    "        \"Developer\",\n",
    "        \"Training: \",\n",
    "        len(train_dev_y),\n",
    "        \"Testing :\",\n",
    "        len(test_dev_y),\n",
    "        \"Combined DEV + TEST\",\n",
    "        len(combineddata_dev_enc),\n",
    "    )\n",
    "\n",
    "    ### BugType Encoder\n",
    "    combineddata_bugtype = train_btype_y + test_btype_y\n",
    "    (\n",
    "        btype_labels_map,\n",
    "        btype_inv_labels_map,\n",
    "        combineddata_bugtype_enc,\n",
    "    ) = create_tag_mapping(combineddata_bugtype, \"Name\")\n",
    "    btype_y_train = combineddata_bugtype_enc[: len(train_btype_y)]\n",
    "    btype_y_test = combineddata_bugtype_enc[len(train_btype_y) :]\n",
    "    print(\n",
    "        \"Bug Type\",\n",
    "        \"Training: \",\n",
    "        len(btype_y_train),\n",
    "        \"Testing :\",\n",
    "        len(btype_y_test),\n",
    "        \"Combined DEV + TEST\",\n",
    "        len(combineddata_bugtype_enc),\n",
    "    )\n",
    "\n",
    "    # /*******************Tokenizer****************************/\n",
    "\n",
    "    x_train_context = [str(row).lower() for row in x_train_context]\n",
    "    x_test_context = [str(row).lower() for row in x_test_context]\n",
    "    combineddata = x_train_context + x_test_context\n",
    "    tk_context = Tokenizer(\n",
    "        num_words=None,\n",
    "        char_level=None,\n",
    "        oov_token=\"Unknown\",\n",
    "        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\d+',\n",
    "    )\n",
    "    tk_context.fit_on_texts(combineddata)\n",
    "\n",
    "    return (\n",
    "        traindata,\n",
    "        tk_context,\n",
    "        x_train_context,\n",
    "        x_train_AST,\n",
    "        dev_y_train,\n",
    "        btype_y_train,\n",
    "        x_test_context,\n",
    "        x_test_AST,\n",
    "        dev_y_test,\n",
    "        btype_y_test,\n",
    "        dev_labels_map,\n",
    "        dev_inv_labels_map,\n",
    "        btype_labels_map,\n",
    "        btype_inv_labels_map,\n",
    "    )\n",
    "\n",
    "\n",
    "def Tokenize(tk_context, x_train_context, x_train_AST, x_test_context, x_test_AST):\n",
    "    # =======================Convert string to index================\n",
    "    # Tokenizer\n",
    "    x_train_context = [str(row).lower() for row in x_train_context]\n",
    "    x_test_context = [str(row).lower() for row in x_test_context]\n",
    "    combineddata_context = x_train_context + x_test_context\n",
    "    tk_context = Tokenizer(\n",
    "        num_words=None,\n",
    "        char_level=None,\n",
    "        oov_token=\"Unknown\",\n",
    "        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    )\n",
    "    tk_context.fit_on_texts(combineddata_context)\n",
    "\n",
    "    x_train_AST = [str(row).lower() for row in x_train_AST]\n",
    "    x_test_AST = [str(row).lower() for row in x_test_AST]\n",
    "    combineddata_AST = x_train_AST + x_test_AST\n",
    "    tk_AST = Tokenizer(num_words=None, char_level=None, oov_token=\"Unknown\")\n",
    "    tk_AST.fit_on_texts(combineddata_AST)\n",
    "\n",
    "    # Convert string to index\n",
    "    x_train_context_sequences = tk_context.texts_to_sequences(x_train_context)\n",
    "    x_train_AST_sequences = tk_AST.texts_to_sequences(x_train_AST)\n",
    "    x_test_context_sequences = tk_context.texts_to_sequences(x_test_context)\n",
    "    x_test_AST_sequences = tk_AST.texts_to_sequences(x_test_AST)\n",
    "\n",
    "    # Padding\n",
    "    x_train_context = pad_sequences(\n",
    "        x_train_context_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\"\n",
    "    )\n",
    "    x_train_AST = pad_sequences(\n",
    "        x_train_AST_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\"\n",
    "    )\n",
    "    x_test_context = pad_sequences(\n",
    "        x_test_context_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\"\n",
    "    )\n",
    "    x_test_AST = pad_sequences(\n",
    "        x_test_AST_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\"\n",
    "    )\n",
    "\n",
    "    # Convert to numpy array\n",
    "    x_train_context = np.array(x_train_context)\n",
    "    x_train_AST = np.array(x_train_AST)\n",
    "    x_test_context = np.array(x_test_context)\n",
    "    x_test_AST = np.array(x_test_AST)\n",
    "    return tk_context, tk_AST, x_train_context, x_train_AST, x_test_context, x_test_AST\n",
    "\n",
    "\n",
    "## Remove record if does not exist in TrainData\n",
    "def RemoveTestRecordIfNotExistInTrainData(traindata, testdata):\n",
    "    traingroup = (\n",
    "        traindata.groupby([\"Name\", \"FixedByID\"], as_index=True)[\"FixedByID\"]\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "    )\n",
    "    testgroup = (\n",
    "        testdata.groupby([\"Name\", \"FixedByID\"], as_index=True)[\"FixedByID\"]\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "    )\n",
    "    for ind in testgroup.index:\n",
    "        try:\n",
    "            record = traindata[\n",
    "                traindata[\"FixedByID\"].str.match(testgroup[\"FixedByID\"][ind])\n",
    "                & traindata[\"Name\"].str.match(testgroup[\"Name\"][ind])\n",
    "            ]\n",
    "            if len(record) < 1:\n",
    "                print(\"remove from testdata...\")\n",
    "                testdata = testdata.drop(\n",
    "                    testdata[\n",
    "                        testdata[\"FixedByID\"].str.match(testgroup[\"FixedByID\"][ind])\n",
    "                        & testdata[\"Name\"].str.match(testgroup[\"Name\"][ind])\n",
    "                    ].index\n",
    "                )\n",
    "        except:\n",
    "            print(\"An exception occurred index :\", ind)\n",
    "    return testdata\n",
    "\n",
    "\n",
    "# create a one hot encoding for one list of tags\n",
    "def one_hot_encode(tags, mapping):\n",
    "    # create empty vector\n",
    "    encoding = zeros(len(mapping), dtype=\"uint8\")\n",
    "    # mark 1 for each tag in the vector\n",
    "    for tag in tags:\n",
    "        encoding[mapping[tag]] = 1\n",
    "    return encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "023fc7aa-f5a7-4486-8b07-51b4a16a3e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiModel(\n",
    "    project,\n",
    "    btype_labels_map,\n",
    "    dev_labels_map,\n",
    "    btype_y_train,\n",
    "    dev_y_train,\n",
    "    btype_y_test,\n",
    "    dev_y_test,\n",
    "    dataaug,\n",
    "    MAX_SEQUENCE_LENGTH,\n",
    "    EMBEDDING_DIM,\n",
    "):\n",
    "    # Logging\n",
    "    filename = (\n",
    "        \"Multimodel\"\n",
    "        + \"_\"\n",
    "        + project\n",
    "        + \"_\"\n",
    "        + dataaug\n",
    "        + \"_\"\n",
    "        + str(Learningrate)\n",
    "        + \"_\"\n",
    "        + str(EMBEDDING_DIM)\n",
    "        + \"_\"\n",
    "        + str(MAX_SEQUENCE_LENGTH)\n",
    "    )\n",
    "    filelog = open(filename + \".txt\", \"w\")\n",
    "    filelog.write(\"StartTime:\" + str(datetime.now()))\n",
    "    filelog.close()\n",
    "\n",
    "    # Paramaters\n",
    "    VALIDATION_SPLIT = 0.2\n",
    "    noofbugtype = len(btype_labels_map)\n",
    "    noofdev = len(dev_labels_map)\n",
    "    btype_y_train = np.array(btype_y_train)\n",
    "    dev_y_train = np.array(dev_y_train)\n",
    "    btype_y_test = np.array(btype_y_test)\n",
    "    dev_y_test = np.array(dev_y_test)\n",
    "\n",
    "    # Visualize Model\n",
    "    logdir = \"logs/\"\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "    # A model.fit() training loop will check at end of every epoch whether the loss is no longer decreasing, considering the min_delta and patiencez\n",
    "    earlystop = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=3)\n",
    "\n",
    "    starttime = datetime.now()\n",
    "    print(\"Start Time =\", starttime)\n",
    "    print(\"Predict Developer\")\n",
    "    with tf.device('/GPU:0'):\n",
    "        # inputs\n",
    "        input_context = Input(\n",
    "            shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.float32, name=\"Bug_TitleandDescription\"\n",
    "        )  # Bug Title and Description\n",
    "        input_AST = Input(\n",
    "            shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.float32, name=\"Bug_CodeSnippetAST\"\n",
    "        )  # Bug Code Snippet AST\n",
    "\n",
    "        # Context Enconder\n",
    "        emb_Context = Embedding(\n",
    "            input_dim=len(tk_context.word_index) + 2,\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            output_dim=EMBEDDING_DIM,\n",
    "            name=\"Context_Embedding\",\n",
    "        )(input_context)\n",
    "        conv_Context = Conv1D(\n",
    "            filters=64,\n",
    "            kernel_size=2,\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "            name=\"Context_Convolutional_Layer\",\n",
    "        )(emb_Context)\n",
    "        maxpool_Context = GlobalMaxPool1D(name=\"Context_Maxpool_Layer\")(conv_Context)\n",
    "        flatcon = Flatten(name=\"Context_Flatten_Layer\")(maxpool_Context)\n",
    "\n",
    "        # AST Enconder\n",
    "        emb_AST = Embedding(\n",
    "            input_dim=len(tk_AST.word_index) + 2,\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            output_dim=EMBEDDING_DIM,\n",
    "            name=\"AST_Embedding\",\n",
    "        )(input_AST)\n",
    "        bilstm_AST = Bidirectional(LSTM(25, return_sequences=True, name=\"AST_LSTM_Layer\"))(\n",
    "            emb_AST\n",
    "        )\n",
    "        maxpool_AST = GlobalMaxPool1D(name=\"AST_Maxpool_Layer\")(bilstm_AST)\n",
    "        flatAST = Flatten(name=\"AST_Flatten_Layer\")(maxpool_AST)\n",
    "\n",
    "        cat = concatenate([flatcon, flatAST], name=\"Concatenate_Flatten_Layer\")\n",
    "\n",
    "        bn = BatchNormalization()(cat)\n",
    "        drop = Dropout(0.5)(bn)\n",
    "        dense = Dense(50, activation=\"relu\")(drop)\n",
    "        DevOutput = Dense(noofdev, activation=\"sigmoid\", name=\"Developer\")(dense)\n",
    "        BugTypeOutput = Dense(noofbugtype, activation=\"sigmoid\", name=\"Bug_Type\")(dense)\n",
    "        Bil_LSTM_MultiTask_model = Model(\n",
    "            inputs=[input_context, input_AST], outputs=[DevOutput, BugTypeOutput]\n",
    "        )\n",
    "\n",
    "        Bil_LSTM_MultiTask_model.compile(\n",
    "            loss=\"binary_crossentropy\",\n",
    "            optimizer=Adam(Learningrate),\n",
    "            metrics=[\n",
    "                \"accuracy\",\n",
    "                c_precision,\n",
    "                c_recall,\n",
    "                \"AUC\",\n",
    "                c_f1_macro,\n",
    "                c_fbeta,\n",
    "                hammingloss,\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        Bil_LSTM_MultiTask_model.fit(\n",
    "            [x_train_context, x_train_AST],\n",
    "            [dev_y_train, btype_y_train],\n",
    "            callbacks=[tensorboard_callback, earlystop],\n",
    "            epochs=50,\n",
    "            verbose=2,\n",
    "            validation_split=0.1,\n",
    "        )\n",
    "\n",
    "    ## Evaluate RNN Single Model\n",
    "        csv_logger = CSVLogger(filename + \".csv\", append=True, separator=\";\")\n",
    "        history = Bil_LSTM_MultiTask_model.evaluate(\n",
    "            [x_test_context, x_test_AST], [dev_y_test, btype_y_test], callbacks=[csv_logger]\n",
    "        )\n",
    "    return history\n",
    "    # filelog =  open(filename + \".txt\",\"a\")\n",
    "    # filelog.write(\"Endtime:\" + str(datetime.now()))\n",
    "    # filelog.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04785194-9ac7-46de-baed-1ff4f4505480",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data \"19/01/2016\" doesn't match format \"%m/%d/%Y\", at position 1. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m\n\u001b[1;32m     20\u001b[0m totaldata \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\n\u001b[1;32m     21\u001b[0m     DataFilePath \u001b[39m+\u001b[39m DataFileName \u001b[39m+\u001b[39m FileType,\n\u001b[1;32m     22\u001b[0m     \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     low_memory\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     27\u001b[0m )\u001b[39m.\u001b[39msample(frac\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m totaldata \u001b[39m=\u001b[39m totaldata\u001b[39m.\u001b[39mrename(columns\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mï»¿RepoID\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mRepoID\u001b[39m\u001b[39m\"\u001b[39m}, inplace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 29\u001b[0m totaldata[\u001b[39m\"\u001b[39m\u001b[39mCreatedDate\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mto_datetime(totaldata[\u001b[39m\"\u001b[39;49m\u001b[39mCreatedDate\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     30\u001b[0m totaldata \u001b[39m=\u001b[39m totaldata\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCreatedDate\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     31\u001b[0m \u001b[39m# totaldata[totaldata['FixedByID'] != 'unknown']\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m \u001b[39m# Cross Validation in TimeSeries\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/tools/datetimes.py:1050\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1048\u001b[0m         result \u001b[39m=\u001b[39m arg\u001b[39m.\u001b[39mmap(cache_array)\n\u001b[1;32m   1049\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1050\u001b[0m         values \u001b[39m=\u001b[39m convert_listlike(arg\u001b[39m.\u001b[39;49m_values, \u001b[39mformat\u001b[39;49m)\n\u001b[1;32m   1051\u001b[0m         result \u001b[39m=\u001b[39m arg\u001b[39m.\u001b[39m_constructor(values, index\u001b[39m=\u001b[39marg\u001b[39m.\u001b[39mindex, name\u001b[39m=\u001b[39marg\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1052\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[39m.\u001b[39mMutableMapping)):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/tools/datetimes.py:453\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[39m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmixed\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 453\u001b[0m     \u001b[39mreturn\u001b[39;00m _array_strptime_with_fallback(arg, name, utc, \u001b[39mformat\u001b[39;49m, exact, errors)\n\u001b[1;32m    455\u001b[0m result, tz_parsed \u001b[39m=\u001b[39m objects_to_datetime64ns(\n\u001b[1;32m    456\u001b[0m     arg,\n\u001b[1;32m    457\u001b[0m     dayfirst\u001b[39m=\u001b[39mdayfirst,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    461\u001b[0m     allow_object\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    462\u001b[0m )\n\u001b[1;32m    464\u001b[0m \u001b[39mif\u001b[39;00m tz_parsed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    465\u001b[0m     \u001b[39m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     \u001b[39m# is in UTC\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/tools/datetimes.py:484\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[0;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_array_strptime_with_fallback\u001b[39m(\n\u001b[1;32m    474\u001b[0m     arg,\n\u001b[1;32m    475\u001b[0m     name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    479\u001b[0m     errors: \u001b[39mstr\u001b[39m,\n\u001b[1;32m    480\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Index:\n\u001b[1;32m    481\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[39m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m     result, timezones \u001b[39m=\u001b[39m array_strptime(arg, fmt, exact\u001b[39m=\u001b[39;49mexact, errors\u001b[39m=\u001b[39;49merrors, utc\u001b[39m=\u001b[39;49mutc)\n\u001b[1;32m    485\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(tz \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m tz \u001b[39min\u001b[39;00m timezones):\n\u001b[1;32m    486\u001b[0m         \u001b[39mreturn\u001b[39;00m _return_parsed_timezone_results(result, timezones, utc, name)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/tslibs/strptime.pyx:530\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/tslibs/strptime.pyx:351\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: time data \"19/01/2016\" doesn't match format \"%m/%d/%Y\", at position 1. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "# Setup Project Parameters\n",
    "# DataAugmentation = True\n",
    "DataAugmentation = False\n",
    "\n",
    "DataAugThreshold = 30000\n",
    "DataFilePath = \"./Data/\"\n",
    "DataFileName = \"IssueelasticsearchWebScrap\"  # Replace with Project File Name\n",
    "FileType = \".csv\"\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "EMBEDDING_DIM = 100\n",
    "LoadDataAugFromFile = False\n",
    "Learningrate = 0.001\n",
    "\n",
    "for DataFileName in os.listdir('./Data'):\n",
    "    if not 'aspnetcore' in DataFileName:\n",
    "        continue\n",
    "    DataFileName = DataFileName.replace('.csv', '')\n",
    "# Loading Data\n",
    "\n",
    "    totaldata = pd.read_csv(\n",
    "        DataFilePath + DataFileName + FileType,\n",
    "        \n",
    "        index_col=False,\n",
    "        dtype=\"unicode\",\n",
    "        encoding=\"latin-1\",\n",
    "        low_memory=False,\n",
    "    ).sample(frac=1)\n",
    "    totaldata = totaldata.rename(columns={\"ï»¿RepoID\": \"RepoID\"}, inplace=False)\n",
    "    totaldata[\"CreatedDate\"] = pd.to_datetime(totaldata[\"CreatedDate\"])\n",
    "    totaldata = totaldata.sort_values(by=[\"CreatedDate\"])\n",
    "    # totaldata[totaldata['FixedByID'] != 'unknown']\n",
    "\n",
    "\n",
    "    # Cross Validation in TimeSeries\n",
    "    totalnoofrecords = len(totaldata)\n",
    "    split = int(totalnoofrecords / 5)\n",
    "    print(split)\n",
    "    mm_res = pd.DataFrame([])\n",
    "    res_metrics = [\n",
    "        \"loss\",\n",
    "        \"Developer_loss\",\n",
    "        \"Bug_Type_loss\",\n",
    "        \"Developer_accuracy\",\n",
    "        \"Developer_c_precision\",\n",
    "        \"Developer_c_recall\",\n",
    "        \"Developer_auc\",\n",
    "        \"Developer_c_f1_macronan\",\n",
    "        \"Developer_c_fbeta\",\n",
    "        \"Developer_hammingloss\",\n",
    "        \"Bug_Type_accuracy\",\n",
    "        \"Bug_Type_c_precision\",\n",
    "        \"Bug_Type_c_recall\",\n",
    "        \"Bug_Type_auc_1\",\n",
    "        \"Bug_Type_c_f1_macronan\",\n",
    "        \"Bug_Type_c_fbeta\",\n",
    "        \"Bug_Type_hammingloss\",\n",
    "    ]\n",
    "    for i in [1, 2, 3, 4, 5]:\n",
    "        # for i in [5]:\n",
    "        currentsplit = split * i\n",
    "        data = totaldata[: int(currentsplit)]\n",
    "        Cleandata(data)\n",
    "        if DataAugmentation == True:\n",
    "            (\n",
    "                traindata,\n",
    "                tk_context,\n",
    "                x_train_context,\n",
    "                x_train_AST,\n",
    "                dev_y_train,\n",
    "                btype_y_train,\n",
    "                x_test_context,\n",
    "                x_test_AST,\n",
    "                dev_y_test,\n",
    "                btype_y_test,\n",
    "                dev_labels_map,\n",
    "                dev_inv_labels_map,\n",
    "                btype_labels_map,\n",
    "                btype_inv_labels_map,\n",
    "            ) = SplitTrainTestDataForMultiLabelWithDataAugmentation(data, str(i))\n",
    "        else:\n",
    "            (\n",
    "                tk_context,\n",
    "                x_train_context,\n",
    "                x_train_AST,\n",
    "                dev_y_train,\n",
    "                btype_y_train,\n",
    "                x_test_context,\n",
    "                x_test_AST,\n",
    "                dev_y_test,\n",
    "                btype_y_test,\n",
    "                dev_labels_map,\n",
    "                dev_inv_labels_map,\n",
    "                btype_labels_map,\n",
    "                btype_inv_labels_map,\n",
    "            ) = SplitTrainTestData(data)\n",
    "        (\n",
    "            tk_context,\n",
    "            tk_AST,\n",
    "            x_train_context,\n",
    "            x_train_AST,\n",
    "            x_test_context,\n",
    "            x_test_AST,\n",
    "        ) = Tokenize(tk_context, x_train_context, x_train_AST, x_test_context, x_test_AST)\n",
    "        history = MultiModel(\n",
    "            DataFileName + \"_30_epoch\",\n",
    "            btype_labels_map,\n",
    "            dev_labels_map,\n",
    "            btype_y_train,\n",
    "            dev_y_train,\n",
    "            btype_y_test,\n",
    "            dev_y_test,\n",
    "            str(DataAugmentation),\n",
    "            MAX_SEQUENCE_LENGTH,\n",
    "            EMBEDDING_DIM,\n",
    "        )\n",
    "        mm_res = pd.concat(\n",
    "            [mm_res, pd.Series(dict(zip(res_metrics, history)))], axis=1, ignore_index=True\n",
    "        )\n",
    "        print(\"finished iteration\", i)\n",
    "    mm_res.T.to_csv(\"./logs/res/\" + DataFileName + \".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06a3e25-5266-4180-ab7e-3077e1cd2997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PredictCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        evaluate = self.model.evaluate([x_test_context, x_test_AST], np.array(dev_y_test))\n",
    "        df_evaluate = pd.DataFrame(columns=['loss','accuracy','c_precision','c_recall', 'auc', 'c_fbeta', 'c_f1_macro', 'hammingloss'])\n",
    "        df_row = {'loss': evaluate[0], 'accuracy': evaluate[1],'c_precision': evaluate[2],\n",
    "                  'c_recall': evaluate[3], 'auc': evaluate[4], 'c_fbeta': evaluate[5],\n",
    "                  'c_f1_macro': evaluate[6], 'hammingloss': evaluate[7]}\n",
    "        print(df_row)\n",
    "\n",
    "\n",
    "\n",
    "def CNNModel(btype_labels_map,dev_labels_map,btype_y_train,dev_y_train,btype_y_test,dev_y_test,splitno,DEV_PREDICT):\n",
    "  from datetime import datetime\n",
    "  #Logging \n",
    "  LR = str(Learningrate).replace('.','')\n",
    "\n",
    "  if DEV_PREDICT == True: \n",
    "    predictwhat = 'DEVPREDICT'\n",
    "  else:\n",
    "    predictwhat = 'BUGPREDICT'\n",
    "\n",
    "  if DataAugmentation == True:\n",
    "   filename = DataFilePath + 'Results/CNNModel' + '_' + DataFileName + '_' + predictwhat + '_' + 'DataAug' + '_' + LR + '_' + str(EMBEDDING_DIM) + '_' + str(MAX_SEQUENCE_LENGTH) + '_S' +  str(splitno)\n",
    "  else:\n",
    "   filename = DataFilePath +'Results/CNNModel' + '_' + DataFileName + '_' + predictwhat + '_' + LR + '_' + str(EMBEDDING_DIM) + '_' + str(MAX_SEQUENCE_LENGTH) + '_S' + str(splitno)\n",
    "\n",
    "  filelog = open(filename + \".txt\", \"w\")\n",
    "  filelog.write(\"StartTime:\" + str(datetime.now()))\n",
    "  filelog.close()\n",
    "  \n",
    "  VALIDATION_SPLIT = 0.2\n",
    "  noofbugtype = len(btype_labels_map)\n",
    "  noofdev = len(dev_labels_map)\n",
    "  btype_y_train = np.array(btype_y_train)\n",
    "  dev_y_train = np.array(dev_y_train)\n",
    "  btype_y_test = np.array(btype_y_test)\n",
    "  dev_y_test = np.array(dev_y_test)\n",
    "\n",
    "  #Visualize Model\n",
    "  logdir = \"logs/\"\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "  #A model.fit() training loop will check at end of every epoch whether the loss is no longer decreasing, considering the min_delta and patiencez\n",
    "  earlystop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "  if DEV_PREDICT == True:\n",
    "    print('Predict Developer')\n",
    "      # inputs\n",
    "    input_context = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.float32, name=\"Bug_TitleandDescription\") #Bug Title and Description \n",
    "    input_AST = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.float32, name =\"Bug_CodeSnippetAST\") #Bug Code Snippet AST\n",
    "    emb_Context = Embedding(input_dim=len(tk_context.word_index) + 2, input_length=MAX_SEQUENCE_LENGTH, output_dim= EMBEDDING_DIM)(input_context)\n",
    "    emb_AST = Embedding(input_dim=len(tk_AST.word_index) + 2, input_length=MAX_SEQUENCE_LENGTH, output_dim= EMBEDDING_DIM)(input_AST)\n",
    "    cat = concatenate([emb_Context, emb_AST])\n",
    "    sd = SpatialDropout1D(0.5)(cat)\n",
    "    con = Conv1D(filters=100, kernel_size=4, padding='same', activation='relu')(sd)\n",
    "    bn = BatchNormalization()(con)\n",
    "    drop = Dropout(0.5)(bn)\n",
    "    maxpool = GlobalMaxPool1D()(drop)\n",
    "    dense = Dense(50,activation = 'relu')(maxpool)\n",
    "    final = Dense(noofdev, activation = 'sigmoid') (dense)\n",
    "    CNN_model = Model(inputs=[input_context, input_AST], outputs=[final])\n",
    "    ## Evaluate CNN Single Model \n",
    "    CNN_model.compile(loss='binary_crossentropy', optimizer=Adam(Learningrate), metrics=['accuracy',c_precision,c_recall,'AUC',c_fbeta, c_f1_macro, hammingloss])\n",
    "    csv_logger = CSVLogger(filename + '.csv', append=True, separator=',')\n",
    "    history = CNN_model.fit([x_train_context, x_train_AST], \n",
    "              dev_y_train, \n",
    "              callbacks=[tensorboard_callback,earlystop,csv_logger,PredictCallback()],\n",
    "              epochs=15, verbose=2, validation_split= 0.1)\n",
    "    \n",
    "    ## Evaluate CNN Single Model \n",
    "    evaluate = CNN_model.evaluate([x_test_context, x_test_AST], dev_y_test)\n",
    "    df_evaluate = pd.DataFrame(columns=['loss','accuracy','c_precision','c_recall', 'auc', 'c_fbeta', 'c_f1_macro', 'hammingloss'])\n",
    "    df_row = {'loss': evaluate[0], 'accuracy': evaluate[1],'c_precision': evaluate[2],\n",
    "              'c_recall': evaluate[3], 'auc': evaluate[4], 'c_fbeta': evaluate[5],\n",
    "              'c_f1_macro': evaluate[6], 'hammingloss': evaluate[7]}\n",
    "    df_evaluate= df_evaluate.append(df_row, ignore_index=True)\n",
    "    df_evaluate.to_csv(filename + \"_eval.csv\")\n",
    "\n",
    "    filelog =  open(filename + \".txt\",\"a\")\n",
    "    filelog.write(\"Endtime:\" + str(datetime.now()))\n",
    "    filelog.close() \n",
    "\n",
    "  else: \n",
    "    print('Predict Bug Type')\n",
    "    # inputs\n",
    "    input_context = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.float32, name=\"Bug_TitleandDescription\") #Bug Title and Description \n",
    "    input_AST = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.float32, name =\"Bug_CodeSnippetAST\") #Bug Code Snippet AST\n",
    "    emb_Context = Embedding(input_dim=len(tk_context.word_index) + 2, input_length=MAX_SEQUENCE_LENGTH, output_dim= EMBEDDING_DIM)(input_context)\n",
    "    emb_AST = Embedding(input_dim=len(tk_AST.word_index) + 2, input_length=MAX_SEQUENCE_LENGTH, output_dim= EMBEDDING_DIM)(input_AST)\n",
    "    cat = concatenate([emb_Context, emb_AST])\n",
    "    sd = SpatialDropout1D(0.5)(cat)\n",
    "    con = Conv1D(filters=100, kernel_size=4, padding='same', activation='relu')(sd)\n",
    "    bn = BatchNormalization()(con)\n",
    "    drop = Dropout(0.5)(bn)\n",
    "    maxpool = GlobalMaxPool1D()(drop)\n",
    "    dense = Dense(50,activation = 'relu')(maxpool)\n",
    "    final = Dense(noofbugtype, activation = 'sigmoid') (dense)\n",
    "    CNN_model = Model(inputs=[input_context, input_AST], outputs=[final])\n",
    "    ## Evaluate CNN Single Model \n",
    "    CNN_model.compile(loss='binary_crossentropy', optimizer=Adam(Learningrate), metrics=['accuracy',c_precision,c_recall,'AUC',c_fbeta, c_f1_macro, hammingloss])\n",
    "    csv_logger = CSVLogger(filename + '.csv', append=True, separator=',')\n",
    "    history = CNN_model.fit([x_train_context, x_train_AST], \n",
    "              btype_y_train, \n",
    "              callbacks=[tensorboard_callback,earlystop,csv_logger],\n",
    "              epochs=50, verbose=2, validation_split= 0.1)\n",
    "    \n",
    "    ## Evaluate RNN Single Model \n",
    "    # evaluate = CNN_model.evaluate([x_test_context, x_test_AST], btype_y_test)\n",
    "    evaluate = CNN_model.evaluate([x_train_context, x_train_AST], btype_y_train)\n",
    "    df_evaluate = pd.DataFrame(columns=['loss','accuracy','c_precision','c_recall', 'auc', 'c_fbeta', 'c_f1_macro', 'hammingloss'])\n",
    "    df_row = {'loss': evaluate[0], 'accuracy': evaluate[1],'c_precision': evaluate[2],\n",
    "              'c_recall': evaluate[3], 'auc': evaluate[4], 'c_fbeta': evaluate[5],\n",
    "              'c_f1_macro': evaluate[6], 'hammingloss': evaluate[7]}\n",
    "    df_evaluate= df_evaluate.append(df_row, ignore_index=True)\n",
    "    df_evaluate.to_csv(filename + \"_eval.csv\")\n",
    "\n",
    "    filelog =  open(filename + \".txt\",\"a\")\n",
    "    filelog.write(\"Endtime:\" + str(datetime.now()))\n",
    "    filelog.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1308649-211d-4291-9495-cdabdf376a94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Bug Type\n",
      "Epoch 1/50\n",
      "27/27 - 1s - loss: 0.3686 - accuracy: 0.3090 - c_precision: 0.2702 - c_recall: 0.3006 - auc: 0.7032 - c_fbeta: 0.2824 - c_f1_macro: nan - hammingloss: 0.1240 - val_loss: 0.6207 - val_accuracy: 0.0430 - val_c_precision: 0.0404 - val_c_recall: 0.0318 - val_auc: 0.6519 - val_c_fbeta: 0.0254 - val_c_f1_macro: nan - val_hammingloss: 0.1246 - 1s/epoch - 53ms/step\n",
      "Epoch 2/50\n",
      "27/27 - 0s - loss: 0.1477 - accuracy: 0.3737 - c_precision: 0.3992 - c_recall: 0.1915 - auc: 0.8612 - c_fbeta: 0.1968 - c_f1_macro: nan - hammingloss: 0.0496 - val_loss: 0.5949 - val_accuracy: 0.0538 - val_c_precision: 0.0873 - val_c_recall: 0.0263 - val_auc: 0.7605 - val_c_fbeta: 0.0227 - val_c_f1_macro: nan - val_hammingloss: 0.0874 - 213ms/epoch - 8ms/step\n",
      "Epoch 3/50\n",
      "27/27 - 0s - loss: 0.1293 - accuracy: 0.4323 - c_precision: 0.5203 - c_recall: 0.2504 - auc: 0.8942 - c_fbeta: 0.2639 - c_f1_macro: nan - hammingloss: 0.0463 - val_loss: 0.5808 - val_accuracy: 0.1183 - val_c_precision: 0.0641 - val_c_recall: 0.0218 - val_auc: 0.7759 - val_c_fbeta: 0.0323 - val_c_f1_macro: nan - val_hammingloss: 0.0856 - 215ms/epoch - 8ms/step\n",
      "Epoch 4/50\n",
      "27/27 - 0s - loss: 0.1238 - accuracy: 0.4216 - c_precision: 0.5236 - c_recall: 0.2293 - auc: 0.9071 - c_fbeta: 0.2458 - c_f1_macro: nan - hammingloss: 0.0447 - val_loss: 0.5745 - val_accuracy: 0.0753 - val_c_precision: 0.0000e+00 - val_c_recall: 0.0000e+00 - val_auc: 0.7767 - val_c_fbeta: 0.0000e+00 - val_c_f1_macro: nan - val_hammingloss: 0.0699 - 214ms/epoch - 8ms/step\n",
      "Epoch 5/50\n",
      "27/27 - 0s - loss: 0.1165 - accuracy: 0.4683 - c_precision: 0.6356 - c_recall: 0.2626 - auc: 0.9205 - c_fbeta: 0.2795 - c_f1_macro: nan - hammingloss: 0.0418 - val_loss: 0.5644 - val_accuracy: 0.1290 - val_c_precision: 0.0000e+00 - val_c_recall: 0.0000e+00 - val_auc: 0.7890 - val_c_fbeta: 0.0000e+00 - val_c_f1_macro: nan - val_hammingloss: 0.0708 - 206ms/epoch - 8ms/step\n",
      "Epoch 6/50\n",
      "27/27 - 0s - loss: 0.1113 - accuracy: 0.5186 - c_precision: 0.6726 - c_recall: 0.3036 - auc: 0.9267 - c_fbeta: 0.3209 - c_f1_macro: nan - hammingloss: 0.0393 - val_loss: 0.5519 - val_accuracy: 0.1398 - val_c_precision: 0.2778 - val_c_recall: 0.0309 - val_auc: 0.8074 - val_c_fbeta: 0.0346 - val_c_f1_macro: nan - val_hammingloss: 0.0726 - 206ms/epoch - 8ms/step\n",
      "Epoch 7/50\n",
      "27/27 - 0s - loss: 0.1048 - accuracy: 0.5281 - c_precision: 0.6962 - c_recall: 0.3550 - auc: 0.9368 - c_fbeta: 0.3754 - c_f1_macro: nan - hammingloss: 0.0384 - val_loss: 0.5407 - val_accuracy: 0.1935 - val_c_precision: 0.4074 - val_c_recall: 0.0725 - val_auc: 0.8122 - val_c_fbeta: 0.0876 - val_c_f1_macro: nan - val_hammingloss: 0.0712 - 208ms/epoch - 8ms/step\n",
      "Epoch 8/50\n",
      "27/27 - 0s - loss: 0.0959 - accuracy: 0.5784 - c_precision: 0.7517 - c_recall: 0.4117 - auc: 0.9468 - c_fbeta: 0.4342 - c_f1_macro: nan - hammingloss: 0.0342 - val_loss: 0.5231 - val_accuracy: 0.1935 - val_c_precision: 0.4263 - val_c_recall: 0.0618 - val_auc: 0.8092 - val_c_fbeta: 0.0739 - val_c_f1_macro: nan - val_hammingloss: 0.0699 - 205ms/epoch - 8ms/step\n",
      "Epoch 9/50\n",
      "27/27 - 0s - loss: 0.0879 - accuracy: 0.6228 - c_precision: 0.7889 - c_recall: 0.4687 - auc: 0.9565 - c_fbeta: 0.4897 - c_f1_macro: nan - hammingloss: 0.0311 - val_loss: 0.5118 - val_accuracy: 0.1935 - val_c_precision: 0.3517 - val_c_recall: 0.0957 - val_auc: 0.8134 - val_c_fbeta: 0.1121 - val_c_f1_macro: nan - val_hammingloss: 0.0735 - 206ms/epoch - 8ms/step\n",
      "Epoch 10/50\n",
      "27/27 - 0s - loss: 0.0804 - accuracy: 0.6766 - c_precision: 0.8143 - c_recall: 0.5105 - auc: 0.9638 - c_fbeta: 0.5379 - c_f1_macro: nan - hammingloss: 0.0285 - val_loss: 0.4949 - val_accuracy: 0.2043 - val_c_precision: 0.3665 - val_c_recall: 0.1225 - val_auc: 0.8308 - val_c_fbeta: 0.1485 - val_c_f1_macro: nan - val_hammingloss: 0.0730 - 206ms/epoch - 8ms/step\n",
      "Epoch 11/50\n",
      "27/27 - 0s - loss: 0.0725 - accuracy: 0.7054 - c_precision: 0.8559 - c_recall: 0.5610 - auc: 0.9688 - c_fbeta: 0.5861 - c_f1_macro: nan - hammingloss: 0.0250 - val_loss: 0.4695 - val_accuracy: 0.2366 - val_c_precision: 0.4678 - val_c_recall: 0.1623 - val_auc: 0.8387 - val_c_fbeta: 0.2042 - val_c_f1_macro: nan - val_hammingloss: 0.0694 - 208ms/epoch - 8ms/step\n",
      "Epoch 12/50\n",
      "27/27 - 0s - loss: 0.0667 - accuracy: 0.7281 - c_precision: 0.8611 - c_recall: 0.6228 - auc: 0.9736 - c_fbeta: 0.6525 - c_f1_macro: nan - hammingloss: 0.0225 - val_loss: 0.4494 - val_accuracy: 0.2258 - val_c_precision: 0.4740 - val_c_recall: 0.1557 - val_auc: 0.8433 - val_c_fbeta: 0.1987 - val_c_f1_macro: nan - val_hammingloss: 0.0690 - 212ms/epoch - 8ms/step\n",
      "Epoch 13/50\n",
      "27/27 - 0s - loss: 0.0596 - accuracy: 0.7605 - c_precision: 0.8775 - c_recall: 0.6526 - auc: 0.9823 - c_fbeta: 0.6732 - c_f1_macro: nan - hammingloss: 0.0208 - val_loss: 0.4159 - val_accuracy: 0.2366 - val_c_precision: 0.4410 - val_c_recall: 0.1510 - val_auc: 0.8239 - val_c_fbeta: 0.1801 - val_c_f1_macro: nan - val_hammingloss: 0.0708 - 210ms/epoch - 8ms/step\n",
      "Epoch 14/50\n",
      "27/27 - 0s - loss: 0.0524 - accuracy: 0.7976 - c_precision: 0.9054 - c_recall: 0.6949 - auc: 0.9861 - c_fbeta: 0.7240 - c_f1_macro: nan - hammingloss: 0.0178 - val_loss: 0.3887 - val_accuracy: 0.2258 - val_c_precision: 0.5071 - val_c_recall: 0.2371 - val_auc: 0.8469 - val_c_fbeta: 0.2868 - val_c_f1_macro: nan - val_hammingloss: 0.0681 - 209ms/epoch - 8ms/step\n",
      "Epoch 15/50\n",
      "27/27 - 0s - loss: 0.0476 - accuracy: 0.8024 - c_precision: 0.8952 - c_recall: 0.7210 - auc: 0.9865 - c_fbeta: 0.7446 - c_f1_macro: nan - hammingloss: 0.0171 - val_loss: 0.3585 - val_accuracy: 0.1828 - val_c_precision: 0.4237 - val_c_recall: 0.1786 - val_auc: 0.8464 - val_c_fbeta: 0.2183 - val_c_f1_macro: nan - val_hammingloss: 0.0730 - 208ms/epoch - 8ms/step\n",
      "Epoch 16/50\n",
      "27/27 - 0s - loss: 0.0423 - accuracy: 0.8359 - c_precision: 0.9150 - c_recall: 0.7723 - auc: 0.9907 - c_fbeta: 0.7978 - c_f1_macro: nan - hammingloss: 0.0144 - val_loss: 0.3293 - val_accuracy: 0.1935 - val_c_precision: 0.4406 - val_c_recall: 0.1908 - val_auc: 0.8271 - val_c_fbeta: 0.2301 - val_c_f1_macro: nan - val_hammingloss: 0.0721 - 209ms/epoch - 8ms/step\n",
      "Epoch 17/50\n",
      "27/27 - 0s - loss: 0.0374 - accuracy: 0.8479 - c_precision: 0.9286 - c_recall: 0.8020 - auc: 0.9927 - c_fbeta: 0.8205 - c_f1_macro: nan - hammingloss: 0.0126 - val_loss: 0.3084 - val_accuracy: 0.1828 - val_c_precision: 0.4339 - val_c_recall: 0.1775 - val_auc: 0.8405 - val_c_fbeta: 0.2119 - val_c_f1_macro: nan - val_hammingloss: 0.0721 - 208ms/epoch - 8ms/step\n",
      "Epoch 18/50\n",
      "27/27 - 0s - loss: 0.0342 - accuracy: 0.8683 - c_precision: 0.9248 - c_recall: 0.8142 - auc: 0.9940 - c_fbeta: 0.8331 - c_f1_macro: nan - hammingloss: 0.0118 - val_loss: 0.2784 - val_accuracy: 0.2043 - val_c_precision: 0.4176 - val_c_recall: 0.2193 - val_auc: 0.8394 - val_c_fbeta: 0.2664 - val_c_f1_macro: nan - val_hammingloss: 0.0748 - 207ms/epoch - 8ms/step\n",
      "Epoch 19/50\n",
      "27/27 - 0s - loss: 0.0303 - accuracy: 0.8719 - c_precision: 0.9349 - c_recall: 0.8308 - auc: 0.9959 - c_fbeta: 0.8585 - c_f1_macro: nan - hammingloss: 0.0109 - val_loss: 0.2605 - val_accuracy: 0.1720 - val_c_precision: 0.4129 - val_c_recall: 0.1719 - val_auc: 0.8209 - val_c_fbeta: 0.2059 - val_c_f1_macro: nan - val_hammingloss: 0.0735 - 210ms/epoch - 8ms/step\n",
      "Epoch 20/50\n",
      "27/27 - 0s - loss: 0.0277 - accuracy: 0.8766 - c_precision: 0.9419 - c_recall: 0.8365 - auc: 0.9973 - c_fbeta: 0.8538 - c_f1_macro: nan - hammingloss: 0.0104 - val_loss: 0.2444 - val_accuracy: 0.1398 - val_c_precision: 0.3580 - val_c_recall: 0.1466 - val_auc: 0.8273 - val_c_fbeta: 0.1679 - val_c_f1_macro: nan - val_hammingloss: 0.0766 - 217ms/epoch - 8ms/step\n",
      "Epoch 21/50\n",
      "27/27 - 0s - loss: 0.0235 - accuracy: 0.8994 - c_precision: 0.9514 - c_recall: 0.8745 - auc: 0.9969 - c_fbeta: 0.8841 - c_f1_macro: nan - hammingloss: 0.0079 - val_loss: 0.2260 - val_accuracy: 0.2043 - val_c_precision: 0.4919 - val_c_recall: 0.2246 - val_auc: 0.8185 - val_c_fbeta: 0.2749 - val_c_f1_macro: nan - val_hammingloss: 0.0690 - 209ms/epoch - 8ms/step\n",
      "Epoch 22/50\n",
      "27/27 - 0s - loss: 0.0217 - accuracy: 0.8958 - c_precision: 0.9463 - c_recall: 0.8998 - auc: 0.9978 - c_fbeta: 0.9067 - c_f1_macro: nan - hammingloss: 0.0073 - val_loss: 0.2169 - val_accuracy: 0.1935 - val_c_precision: 0.4459 - val_c_recall: 0.2171 - val_auc: 0.8267 - val_c_fbeta: 0.2592 - val_c_f1_macro: nan - val_hammingloss: 0.0721 - 206ms/epoch - 8ms/step\n",
      "Epoch 23/50\n",
      "27/27 - 0s - loss: 0.0207 - accuracy: 0.9006 - c_precision: 0.9633 - c_recall: 0.8995 - auc: 0.9978 - c_fbeta: 0.9155 - c_f1_macro: nan - hammingloss: 0.0065 - val_loss: 0.2116 - val_accuracy: 0.2043 - val_c_precision: 0.4677 - val_c_recall: 0.2248 - val_auc: 0.8323 - val_c_fbeta: 0.2797 - val_c_f1_macro: nan - val_hammingloss: 0.0708 - 213ms/epoch - 8ms/step\n",
      "Epoch 24/50\n",
      "27/27 - 0s - loss: 0.0196 - accuracy: 0.9090 - c_precision: 0.9514 - c_recall: 0.9020 - auc: 0.9985 - c_fbeta: 0.9169 - c_f1_macro: nan - hammingloss: 0.0070 - val_loss: 0.2190 - val_accuracy: 0.1720 - val_c_precision: 0.4349 - val_c_recall: 0.2126 - val_auc: 0.8126 - val_c_fbeta: 0.2652 - val_c_f1_macro: nan - val_hammingloss: 0.0735 - 207ms/epoch - 8ms/step\n",
      "Epoch 25/50\n",
      "27/27 - 0s - loss: 0.0183 - accuracy: 0.9090 - c_precision: 0.9679 - c_recall: 0.8991 - auc: 0.9971 - c_fbeta: 0.9161 - c_f1_macro: nan - hammingloss: 0.0063 - val_loss: 0.2164 - val_accuracy: 0.1720 - val_c_precision: 0.3750 - val_c_recall: 0.1675 - val_auc: 0.8215 - val_c_fbeta: 0.2013 - val_c_f1_macro: nan - val_hammingloss: 0.0766 - 206ms/epoch - 8ms/step\n",
      "Epoch 26/50\n",
      "27/27 - 0s - loss: 0.0159 - accuracy: 0.9150 - c_precision: 0.9571 - c_recall: 0.9237 - auc: 0.9992 - c_fbeta: 0.9329 - c_f1_macro: nan - hammingloss: 0.0057 - val_loss: 0.2192 - val_accuracy: 0.1613 - val_c_precision: 0.3785 - val_c_recall: 0.1862 - val_auc: 0.8288 - val_c_fbeta: 0.2267 - val_c_f1_macro: nan - val_hammingloss: 0.0771 - 208ms/epoch - 8ms/step\n",
      "Epoch 27/50\n",
      "27/27 - 0s - loss: 0.0158 - accuracy: 0.9234 - c_precision: 0.9586 - c_recall: 0.9153 - auc: 0.9984 - c_fbeta: 0.9348 - c_f1_macro: nan - hammingloss: 0.0054 - val_loss: 0.2191 - val_accuracy: 0.1613 - val_c_precision: 0.4499 - val_c_recall: 0.1984 - val_auc: 0.8271 - val_c_fbeta: 0.2369 - val_c_f1_macro: nan - val_hammingloss: 0.0717 - 205ms/epoch - 8ms/step\n",
      "Epoch 28/50\n",
      "27/27 - 0s - loss: 0.0150 - accuracy: 0.9126 - c_precision: 0.9790 - c_recall: 0.9345 - auc: 0.9991 - c_fbeta: 0.9400 - c_f1_macro: nan - hammingloss: 0.0041 - val_loss: 0.2288 - val_accuracy: 0.2043 - val_c_precision: 0.4289 - val_c_recall: 0.2289 - val_auc: 0.8267 - val_c_fbeta: 0.2779 - val_c_f1_macro: nan - val_hammingloss: 0.0744 - 207ms/epoch - 8ms/step\n",
      "Epoch 29/50\n",
      "27/27 - 0s - loss: 0.0143 - accuracy: 0.9162 - c_precision: 0.9643 - c_recall: 0.9187 - auc: 0.9993 - c_fbeta: 0.9282 - c_f1_macro: nan - hammingloss: 0.0050 - val_loss: 0.2248 - val_accuracy: 0.2151 - val_c_precision: 0.4699 - val_c_recall: 0.2202 - val_auc: 0.8350 - val_c_fbeta: 0.2753 - val_c_f1_macro: nan - val_hammingloss: 0.0708 - 207ms/epoch - 8ms/step\n",
      "Epoch 30/50\n",
      "27/27 - 0s - loss: 0.0135 - accuracy: 0.9186 - c_precision: 0.9646 - c_recall: 0.9426 - auc: 0.9988 - c_fbeta: 0.9435 - c_f1_macro: nan - hammingloss: 0.0045 - val_loss: 0.2369 - val_accuracy: 0.1935 - val_c_precision: 0.4534 - val_c_recall: 0.1862 - val_auc: 0.8150 - val_c_fbeta: 0.2266 - val_c_f1_macro: nan - val_hammingloss: 0.0717 - 206ms/epoch - 8ms/step\n",
      "Epoch 31/50\n",
      "27/27 - 0s - loss: 0.0115 - accuracy: 0.9293 - c_precision: 0.9661 - c_recall: 0.9476 - auc: 0.9995 - c_fbeta: 0.9555 - c_f1_macro: nan - hammingloss: 0.0042 - val_loss: 0.2343 - val_accuracy: 0.1613 - val_c_precision: 0.4147 - val_c_recall: 0.1699 - val_auc: 0.8283 - val_c_fbeta: 0.1985 - val_c_f1_macro: nan - val_hammingloss: 0.0730 - 207ms/epoch - 8ms/step\n",
      "Epoch 32/50\n",
      "27/27 - 0s - loss: 0.0122 - accuracy: 0.9234 - c_precision: 0.9744 - c_recall: 0.9456 - auc: 0.9989 - c_fbeta: 0.9501 - c_f1_macro: nan - hammingloss: 0.0039 - val_loss: 0.2493 - val_accuracy: 0.2151 - val_c_precision: 0.4296 - val_c_recall: 0.2202 - val_auc: 0.8170 - val_c_fbeta: 0.2665 - val_c_f1_macro: nan - val_hammingloss: 0.0739 - 206ms/epoch - 8ms/step\n",
      "Epoch 33/50\n",
      "27/27 - 0s - loss: 0.0097 - accuracy: 0.9269 - c_precision: 0.9753 - c_recall: 0.9552 - auc: 0.9997 - c_fbeta: 0.9616 - c_f1_macro: nan - hammingloss: 0.0034 - val_loss: 0.2429 - val_accuracy: 0.1828 - val_c_precision: 0.4349 - val_c_recall: 0.1973 - val_auc: 0.8245 - val_c_fbeta: 0.2303 - val_c_f1_macro: nan - val_hammingloss: 0.0726 - 210ms/epoch - 8ms/step\n",
      "Epoch 34/50\n",
      "27/27 - 0s - loss: 0.0112 - accuracy: 0.9174 - c_precision: 0.9709 - c_recall: 0.9454 - auc: 0.9995 - c_fbeta: 0.9487 - c_f1_macro: nan - hammingloss: 0.0040 - val_loss: 0.2451 - val_accuracy: 0.2151 - val_c_precision: 0.4394 - val_c_recall: 0.2115 - val_auc: 0.8232 - val_c_fbeta: 0.2465 - val_c_f1_macro: nan - val_hammingloss: 0.0726 - 210ms/epoch - 8ms/step\n",
      "Epoch 35/50\n",
      "27/27 - 0s - loss: 0.0095 - accuracy: 0.9222 - c_precision: 0.9797 - c_recall: 0.9542 - auc: 0.9986 - c_fbeta: 0.9582 - c_f1_macro: nan - hammingloss: 0.0032 - val_loss: 0.2533 - val_accuracy: 0.2151 - val_c_precision: 0.4529 - val_c_recall: 0.2037 - val_auc: 0.8097 - val_c_fbeta: 0.2448 - val_c_f1_macro: nan - val_hammingloss: 0.0712 - 208ms/epoch - 8ms/step\n",
      "Epoch 36/50\n",
      "27/27 - 0s - loss: 0.0089 - accuracy: 0.9305 - c_precision: 0.9775 - c_recall: 0.9621 - auc: 0.9997 - c_fbeta: 0.9639 - c_f1_macro: nan - hammingloss: 0.0029 - val_loss: 0.2512 - val_accuracy: 0.1935 - val_c_precision: 0.4576 - val_c_recall: 0.1851 - val_auc: 0.8214 - val_c_fbeta: 0.2182 - val_c_f1_macro: nan - val_hammingloss: 0.0708 - 209ms/epoch - 8ms/step\n",
      "Epoch 37/50\n",
      "27/27 - 0s - loss: 0.0093 - accuracy: 0.9257 - c_precision: 0.9831 - c_recall: 0.9601 - auc: 0.9996 - c_fbeta: 0.9638 - c_f1_macro: nan - hammingloss: 0.0027 - val_loss: 0.2630 - val_accuracy: 0.1828 - val_c_precision: 0.4218 - val_c_recall: 0.2300 - val_auc: 0.8171 - val_c_fbeta: 0.2722 - val_c_f1_macro: nan - val_hammingloss: 0.0739 - 217ms/epoch - 8ms/step\n",
      "Epoch 38/50\n",
      "27/27 - 0s - loss: 0.0082 - accuracy: 0.9222 - c_precision: 0.9764 - c_recall: 0.9684 - auc: 0.9993 - c_fbeta: 0.9702 - c_f1_macro: nan - hammingloss: 0.0026 - val_loss: 0.2711 - val_accuracy: 0.1720 - val_c_precision: 0.4424 - val_c_recall: 0.1643 - val_auc: 0.8065 - val_c_fbeta: 0.1962 - val_c_f1_macro: nan - val_hammingloss: 0.0717 - 207ms/epoch - 8ms/step\n",
      "Epoch 39/50\n",
      "27/27 - 0s - loss: 0.0082 - accuracy: 0.9186 - c_precision: 0.9792 - c_recall: 0.9611 - auc: 0.9998 - c_fbeta: 0.9625 - c_f1_macro: nan - hammingloss: 0.0028 - val_loss: 0.2790 - val_accuracy: 0.1935 - val_c_precision: 0.4600 - val_c_recall: 0.2160 - val_auc: 0.7899 - val_c_fbeta: 0.2570 - val_c_f1_macro: nan - val_hammingloss: 0.0708 - 209ms/epoch - 8ms/step\n",
      "Epoch 40/50\n",
      "27/27 - 0s - loss: 0.0082 - accuracy: 0.9329 - c_precision: 0.9767 - c_recall: 0.9625 - auc: 0.9998 - c_fbeta: 0.9646 - c_f1_macro: nan - hammingloss: 0.0029 - val_loss: 0.2718 - val_accuracy: 0.1613 - val_c_precision: 0.4021 - val_c_recall: 0.1643 - val_auc: 0.8158 - val_c_fbeta: 0.1916 - val_c_f1_macro: nan - val_hammingloss: 0.0739 - 209ms/epoch - 8ms/step\n",
      "Epoch 41/50\n",
      "27/27 - 0s - loss: 0.0073 - accuracy: 0.9401 - c_precision: 0.9839 - c_recall: 0.9661 - auc: 0.9993 - c_fbeta: 0.9665 - c_f1_macro: nan - hammingloss: 0.0024 - val_loss: 0.2833 - val_accuracy: 0.1613 - val_c_precision: 0.4398 - val_c_recall: 0.1719 - val_auc: 0.7959 - val_c_fbeta: 0.1973 - val_c_f1_macro: nan - val_hammingloss: 0.0717 - 211ms/epoch - 8ms/step\n",
      "Epoch 42/50\n",
      "27/27 - 0s - loss: 0.0075 - accuracy: 0.9377 - c_precision: 0.9731 - c_recall: 0.9523 - auc: 0.9987 - c_fbeta: 0.9573 - c_f1_macro: nan - hammingloss: 0.0025 - val_loss: 0.2786 - val_accuracy: 0.1505 - val_c_precision: 0.4090 - val_c_recall: 0.1719 - val_auc: 0.8112 - val_c_fbeta: 0.1973 - val_c_f1_macro: nan - val_hammingloss: 0.0739 - 208ms/epoch - 8ms/step\n",
      "Epoch 43/50\n",
      "27/27 - 0s - loss: 0.0086 - accuracy: 0.9329 - c_precision: 0.9694 - c_recall: 0.9711 - auc: 0.9992 - c_fbeta: 0.9715 - c_f1_macro: nan - hammingloss: 0.0029 - val_loss: 0.2840 - val_accuracy: 0.1828 - val_c_precision: 0.4453 - val_c_recall: 0.1784 - val_auc: 0.7692 - val_c_fbeta: 0.1969 - val_c_f1_macro: nan - val_hammingloss: 0.0712 - 208ms/epoch - 8ms/step\n",
      "Epoch 44/50\n",
      "27/27 - 0s - loss: 0.0077 - accuracy: 0.9293 - c_precision: 0.9798 - c_recall: 0.9637 - auc: 0.9998 - c_fbeta: 0.9638 - c_f1_macro: nan - hammingloss: 0.0027 - val_loss: 0.2890 - val_accuracy: 0.1290 - val_c_precision: 0.4157 - val_c_recall: 0.1973 - val_auc: 0.8131 - val_c_fbeta: 0.2221 - val_c_f1_macro: nan - val_hammingloss: 0.0739 - 212ms/epoch - 8ms/step\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0360 - accuracy: 0.8685 - c_precision: 0.9362 - c_recall: 0.9143 - auc: 0.9791 - c_fbeta: 0.9165 - c_f1_macro: nan - hammingloss: 0.0079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2176299/717167366.py:113: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_evaluate= df_evaluate.append(df_row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "CNNModel(btype_labels_map,dev_labels_map,btype_y_train,dev_y_train,btype_y_test,dev_y_test,5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320cf99f-cff3-47ce-b21e-cf4020510398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 1), (4, 3), (12, 3), (7, 4), (11, 6), (5, 21), (14, 194)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(dev_y_test).argmax(1)\n",
    "unique, counts = np.unique(np.array(dev_y_test).argmax(1), return_counts=True)\n",
    "sorted(dict(zip(unique, counts)).items(), key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343d57ac-91af-4bb5-8b7b-50e2f699de2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Single Task RNN Model\n",
    "def RNNModel(\n",
    "    btype_labels_map,\n",
    "    dev_labels_map,\n",
    "    btype_y_train,\n",
    "    dev_y_train,\n",
    "    btype_y_test,\n",
    "    dev_y_test,\n",
    "    splitno,\n",
    "    DEV_PREDICT,\n",
    "):\n",
    "    from datetime import datetime\n",
    "\n",
    "    # Logging\n",
    "    LR = str(Learningrate).replace(\".\", \"\")\n",
    "\n",
    "    if DEV_PREDICT == True:\n",
    "        predictwhat = \"DEVPREDICT\"\n",
    "    else:\n",
    "        predictwhat = \"BUGPREDICT\"\n",
    "\n",
    "    if DataAugmentation == True:\n",
    "        filename = (\n",
    "            DataFilePath\n",
    "            + \"Results/RNNModel\"\n",
    "            + \"_\"\n",
    "            + DataFileName\n",
    "            + \"_\"\n",
    "            + predictwhat\n",
    "            + \"_\"\n",
    "            + \"DataAug\"\n",
    "            + \"_\"\n",
    "            + LR\n",
    "            + \"_\"\n",
    "            + str(EMBEDDING_DIM)\n",
    "            + \"_\"\n",
    "            + str(MAX_SEQUENCE_LENGTH)\n",
    "            + \"_S\"\n",
    "            + str(splitno)\n",
    "        )\n",
    "    else:\n",
    "        filename = (\n",
    "            DataFilePath\n",
    "            + \"Results/RNNModel\"\n",
    "            + \"_\"\n",
    "            + DataFileName\n",
    "            + \"_\"\n",
    "            + predictwhat\n",
    "            + \"_\"\n",
    "            + LR\n",
    "            + \"_\"\n",
    "            + str(EMBEDDING_DIM)\n",
    "            + \"_\"\n",
    "            + str(MAX_SEQUENCE_LENGTH)\n",
    "            + \"_S\"\n",
    "            + str(splitno)\n",
    "        )\n",
    "\n",
    "    filelog = open(filename + \".txt\", \"w\")\n",
    "    filelog.write(\"StartTime:\" + str(datetime.now()))\n",
    "    filelog.close()\n",
    "\n",
    "    VALIDATION_SPLIT = 0.2\n",
    "    noofbugtype = len(btype_labels_map)\n",
    "    noofdev = len(dev_labels_map)\n",
    "    btype_y_train = np.array(btype_y_train)\n",
    "    dev_y_train = np.array(dev_y_train)\n",
    "    btype_y_test = np.array(btype_y_test)\n",
    "    dev_y_test = np.array(dev_y_test)\n",
    "\n",
    "    # Visualize Model\n",
    "    logdir = \"logs/\"\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "    # A model.fit() training loop will check at end of every epoch whether the loss is no longer decreasing, considering the min_delta and patiencez\n",
    "    earlystop = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=3)\n",
    "\n",
    "    if DEV_PREDICT == True:\n",
    "        print(\"Predict Developer\")\n",
    "        # inputs\n",
    "        input_context = Input(\n",
    "            shape=(MAX_SEQUENCE_LENGTH,),\n",
    "            dtype=tf.float32,\n",
    "            name=\"Bug_TitleandDescription\",\n",
    "        )  # Bug Title and Description\n",
    "        input_AST = Input(\n",
    "            shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.float32, name=\"Bug_CodeSnippetAST\"\n",
    "        )  # Bug Code Snippet AST\n",
    "        emb_Context = Embedding(\n",
    "            input_dim=len(tk_context.word_index) + 2,\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            output_dim=EMBEDDING_DIM,\n",
    "        )(input_context)\n",
    "        emb_AST = Embedding(\n",
    "            input_dim=len(tk_AST.word_index) + 2,\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            output_dim=EMBEDDING_DIM,\n",
    "        )(input_AST)\n",
    "        cat = concatenate([emb_Context, emb_AST])\n",
    "        sd = SpatialDropout1D(0.5)(cat)\n",
    "        # Bidirectional layer will enable our model to predict a missing word in a sequence,\n",
    "        # So, using this feature will enable the model to look at the context on both the left and the right.\n",
    "        bilstm = Bidirectional(LSTM(25, return_sequences=True))(sd)\n",
    "        bn = BatchNormalization()(bilstm)\n",
    "        drop = Dropout(0.5)(bn)\n",
    "        maxpool = GlobalMaxPool1D()(drop)\n",
    "        dense = Dense(50, activation=\"relu\")(maxpool)\n",
    "        final = Dense(noofdev, activation=\"sigmoid\")(dense)\n",
    "        Bil_LSTM_model = Model(inputs=[input_context, input_AST], outputs=[final])\n",
    "\n",
    "        Bil_LSTM_model.compile(\n",
    "            loss=\"binary_crossentropy\",\n",
    "            optimizer=Adam(Learningrate),\n",
    "            metrics=[\n",
    "                \"accuracy\",\n",
    "                c_precision,\n",
    "                c_recall,\n",
    "                \"AUC\",\n",
    "                c_fbeta,\n",
    "                c_f1_macro,\n",
    "                hammingloss,\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        csv_logger = CSVLogger(filename + \".csv\", append=True, separator=\",\")\n",
    "        history = Bil_LSTM_model.fit(\n",
    "            [x_train_context, x_train_AST],\n",
    "            dev_y_train,\n",
    "            callbacks=[tensorboard_callback, earlystop, csv_logger],\n",
    "            epochs=50,\n",
    "            verbose=2,\n",
    "            validation_split=0.1,\n",
    "        )\n",
    "\n",
    "        ## Evaluate RNN Single Model\n",
    "\n",
    "        evaluate = Bil_LSTM_model.evaluate([x_test_context, x_test_AST], dev_y_test)\n",
    "        df_evaluate = pd.DataFrame(\n",
    "            columns=[\n",
    "                \"loss\",\n",
    "                \"accuracy\",\n",
    "                \"c_precision\",\n",
    "                \"c_recall\",\n",
    "                \"auc\",\n",
    "                \"c_fbeta\",\n",
    "                \"c_f1_macro\",\n",
    "                \"hammingloss\",\n",
    "            ]\n",
    "        )\n",
    "        df_row = {\n",
    "            \"loss\": evaluate[0],\n",
    "            \"accuracy\": evaluate[1],\n",
    "            \"c_precision\": evaluate[2],\n",
    "            \"c_recall\": evaluate[3],\n",
    "            \"auc\": evaluate[4],\n",
    "            \"c_fbeta\": evaluate[5],\n",
    "            \"c_f1_macro\": evaluate[6],\n",
    "            \"hammingloss\": evaluate[7],\n",
    "        }\n",
    "        df_evaluate = df_evaluate.append(df_row, ignore_index=True)\n",
    "        df_evaluate.to_csv(filename + \"_eval.csv\")\n",
    "\n",
    "        filelog = open(filename + \".txt\", \"a\")\n",
    "        filelog.write(\"Endtime:\" + str(datetime.now()))\n",
    "        filelog.close()\n",
    "\n",
    "    else:\n",
    "        print(\"Predict Bug Type\")\n",
    "        # inputs\n",
    "        input_context = Input(\n",
    "            shape=(MAX_SEQUENCE_LENGTH,),\n",
    "            dtype=tf.float32,\n",
    "            name=\"Bug_TitleandDescription\",\n",
    "        )  # Bug Title and Description\n",
    "        input_AST = Input(\n",
    "            shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.float32, name=\"Bug_CodeSnippetAST\"\n",
    "        )  # Bug Code Snippet AST\n",
    "        emb_Context = Embedding(\n",
    "            input_dim=len(tk_context.word_index) + 2,\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            output_dim=EMBEDDING_DIM,\n",
    "        )(input_context)\n",
    "        emb_AST = Embedding(\n",
    "            input_dim=len(tk_AST.word_index) + 2,\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            output_dim=EMBEDDING_DIM,\n",
    "        )(input_AST)\n",
    "        cat = concatenate([emb_Context, emb_AST])\n",
    "        sd = SpatialDropout1D(0.5)(cat)\n",
    "        # Bidirectional layer will enable our model to predict a missing word in a sequence,\n",
    "        # So, using this feature will enable the model to look at the context on both the left and the right.\n",
    "        bilstm = Bidirectional(LSTM(25, return_sequences=True))(sd)\n",
    "        bn = BatchNormalization()(bilstm)\n",
    "        drop = Dropout(0.5)(bn)\n",
    "        maxpool = GlobalMaxPool1D()(drop)\n",
    "        dense = Dense(50, activation=\"relu\")(maxpool)\n",
    "        final = Dense(noofbugtype, activation=\"sigmoid\")(dense)\n",
    "        Bil_LSTM_model = Model(inputs=[input_context, input_AST], outputs=[final])\n",
    "\n",
    "        Bil_LSTM_model.compile(\n",
    "            loss=\"binary_crossentropy\",\n",
    "            optimizer=Adam(Learningrate),\n",
    "            metrics=[\n",
    "                \"accuracy\",\n",
    "                c_precision,\n",
    "                c_recall,\n",
    "                \"AUC\",\n",
    "                c_fbeta,\n",
    "                c_f1_macro,\n",
    "                hammingloss,\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        csv_logger_bug = CSVLogger(filename + \".csv\", append=True, separator=\",\")\n",
    "        history = Bil_LSTM_model.fit(\n",
    "            [x_train_context, x_train_AST],\n",
    "            btype_y_train,\n",
    "            callbacks=[tensorboard_callback, earlystop, csv_logger_bug],\n",
    "            epochs=50,\n",
    "            verbose=2,\n",
    "            validation_split=0.1,\n",
    "        )\n",
    "\n",
    "        ## Evaluate RNN Single Model\n",
    "        evaluate = Bil_LSTM_model.evaluate([x_test_context, x_test_AST], btype_y_test)\n",
    "        df_evaluate = pd.DataFrame(\n",
    "            columns=[\n",
    "                \"loss\",\n",
    "                \"accuracy\",\n",
    "                \"c_precision\",\n",
    "                \"c_recall\",\n",
    "                \"auc\",\n",
    "                \"c_fbeta\",\n",
    "                \"c_f1_macro\",\n",
    "                \"hammingloss\",\n",
    "            ]\n",
    "        )\n",
    "        df_row = {\n",
    "            \"loss\": evaluate[0],\n",
    "            \"accuracy\": evaluate[1],\n",
    "            \"c_precision\": evaluate[2],\n",
    "            \"c_recall\": evaluate[3],\n",
    "            \"auc\": evaluate[4],\n",
    "            \"c_fbeta\": evaluate[5],\n",
    "            \"c_f1_macro\": evaluate[6],\n",
    "            \"hammingloss\": evaluate[7],\n",
    "        }\n",
    "        df_evaluate = df_evaluate.append(df_row, ignore_index=True)\n",
    "        df_evaluate.to_csv(filename + \"_eval.csv\")\n",
    "\n",
    "        filelog = open(filename + \".txt\", \"a\")\n",
    "        filelog.write(\"Endtime:\" + str(datetime.now()))\n",
    "        filelog.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8199ea6c-c1d9-4e15-b9d5-8ea12431724a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Developer\n",
      "Epoch 1/50\n",
      "27/27 - 3s - loss: 0.2907 - accuracy: 0.7737 - c_precision: 0.5607 - c_recall: 0.8425 - auc: 0.9021 - c_fbeta: 0.7145 - c_f1_macro: nan - hammingloss: 0.1100 - val_loss: 0.6307 - val_accuracy: 0.8387 - val_c_precision: 0.8220 - val_c_recall: 0.8373 - val_auc: 0.8961 - val_c_fbeta: 0.8343 - val_c_f1_macro: nan - val_hammingloss: 0.0215 - 3s/epoch - 129ms/step\n",
      "Epoch 2/50\n",
      "27/27 - 0s - loss: 0.0867 - accuracy: 0.8467 - c_precision: 0.8396 - c_recall: 0.8323 - auc: 0.9388 - c_fbeta: 0.8349 - c_f1_macro: nan - hammingloss: 0.0198 - val_loss: 0.6007 - val_accuracy: 0.8387 - val_c_precision: 0.8373 - val_c_recall: 0.8373 - val_auc: 0.9499 - val_c_fbeta: 0.8373 - val_c_f1_macro: nan - val_hammingloss: 0.0202 - 456ms/epoch - 17ms/step\n",
      "Epoch 3/50\n",
      "27/27 - 0s - loss: 0.0791 - accuracy: 0.8467 - c_precision: 0.8411 - c_recall: 0.8333 - auc: 0.9581 - c_fbeta: 0.8360 - c_f1_macro: nan - hammingloss: 0.0196 - val_loss: 0.5724 - val_accuracy: 0.8387 - val_c_precision: 0.8373 - val_c_recall: 0.8373 - val_auc: 0.9459 - val_c_fbeta: 0.8373 - val_c_f1_macro: nan - val_hammingloss: 0.0202 - 469ms/epoch - 17ms/step\n",
      "Epoch 4/50\n",
      "27/27 - 0s - loss: 0.0751 - accuracy: 0.8467 - c_precision: 0.8526 - c_recall: 0.8468 - auc: 0.9634 - c_fbeta: 0.8495 - c_f1_macro: nan - hammingloss: 0.0195 - val_loss: 0.5481 - val_accuracy: 0.8387 - val_c_precision: 0.8373 - val_c_recall: 0.8373 - val_auc: 0.9581 - val_c_fbeta: 0.8373 - val_c_f1_macro: nan - val_hammingloss: 0.0202 - 456ms/epoch - 17ms/step\n",
      "Epoch 5/50\n",
      "27/27 - 0s - loss: 0.0707 - accuracy: 0.8467 - c_precision: 0.8538 - c_recall: 0.8478 - auc: 0.9692 - c_fbeta: 0.8507 - c_f1_macro: nan - hammingloss: 0.0193 - val_loss: 0.5189 - val_accuracy: 0.8387 - val_c_precision: 0.8348 - val_c_recall: 0.8258 - val_auc: 0.9628 - val_c_fbeta: 0.8258 - val_c_f1_macro: nan - val_hammingloss: 0.0208 - 457ms/epoch - 17ms/step\n",
      "Epoch 6/50\n",
      "27/27 - 0s - loss: 0.0649 - accuracy: 0.8467 - c_precision: 0.8519 - c_recall: 0.8491 - auc: 0.9752 - c_fbeta: 0.8519 - c_f1_macro: nan - hammingloss: 0.0194 - val_loss: 0.4926 - val_accuracy: 0.8387 - val_c_precision: 0.8341 - val_c_recall: 0.8154 - val_auc: 0.9647 - val_c_fbeta: 0.8154 - val_c_f1_macro: nan - val_hammingloss: 0.0215 - 450ms/epoch - 17ms/step\n",
      "Epoch 7/50\n",
      "27/27 - 0s - loss: 0.0592 - accuracy: 0.8479 - c_precision: 0.8802 - c_recall: 0.8468 - auc: 0.9807 - c_fbeta: 0.8495 - c_f1_macro: nan - hammingloss: 0.0174 - val_loss: 0.4690 - val_accuracy: 0.8387 - val_c_precision: 0.8373 - val_c_recall: 0.8373 - val_auc: 0.9679 - val_c_fbeta: 0.8373 - val_c_f1_macro: nan - val_hammingloss: 0.0202 - 455ms/epoch - 17ms/step\n",
      "Epoch 8/50\n",
      "27/27 - 0s - loss: 0.0495 - accuracy: 0.8587 - c_precision: 0.9046 - c_recall: 0.8368 - auc: 0.9872 - c_fbeta: 0.8393 - c_f1_macro: nan - hammingloss: 0.0150 - val_loss: 0.4346 - val_accuracy: 0.8387 - val_c_precision: 0.8315 - val_c_recall: 0.8039 - val_auc: 0.9690 - val_c_fbeta: 0.8039 - val_c_f1_macro: nan - val_hammingloss: 0.0222 - 467ms/epoch - 17ms/step\n",
      "Epoch 9/50\n",
      "27/27 - 0s - loss: 0.0415 - accuracy: 0.8874 - c_precision: 0.9423 - c_recall: 0.8526 - auc: 0.9918 - c_fbeta: 0.8548 - c_f1_macro: nan - hammingloss: 0.0129 - val_loss: 0.4061 - val_accuracy: 0.8387 - val_c_precision: 0.8366 - val_c_recall: 0.7820 - val_auc: 0.9665 - val_c_fbeta: 0.7820 - val_c_f1_macro: nan - val_hammingloss: 0.0228 - 455ms/epoch - 17ms/step\n",
      "Epoch 10/50\n",
      "27/27 - 0s - loss: 0.0325 - accuracy: 0.9138 - c_precision: 0.9590 - c_recall: 0.8756 - auc: 0.9936 - c_fbeta: 0.8783 - c_f1_macro: nan - hammingloss: 0.0106 - val_loss: 0.3699 - val_accuracy: 0.8387 - val_c_precision: 0.8391 - val_c_recall: 0.7935 - val_auc: 0.9637 - val_c_fbeta: 0.7935 - val_c_f1_macro: nan - val_hammingloss: 0.0222 - 457ms/epoch - 17ms/step\n",
      "Epoch 11/50\n",
      "27/27 - 0s - loss: 0.0272 - accuracy: 0.9257 - c_precision: 0.9792 - c_recall: 0.8860 - auc: 0.9967 - c_fbeta: 0.8887 - c_f1_macro: nan - hammingloss: 0.0086 - val_loss: 0.3317 - val_accuracy: 0.8387 - val_c_precision: 0.8391 - val_c_recall: 0.7935 - val_auc: 0.9588 - val_c_fbeta: 0.7935 - val_c_f1_macro: nan - val_hammingloss: 0.0222 - 456ms/epoch - 17ms/step\n",
      "Epoch 12/50\n",
      "27/27 - 0s - loss: 0.0220 - accuracy: 0.9449 - c_precision: 0.9829 - c_recall: 0.9067 - auc: 0.9977 - c_fbeta: 0.9095 - c_f1_macro: nan - hammingloss: 0.0071 - val_loss: 0.2984 - val_accuracy: 0.8387 - val_c_precision: 0.8415 - val_c_recall: 0.8050 - val_auc: 0.9578 - val_c_fbeta: 0.8050 - val_c_f1_macro: nan - val_hammingloss: 0.0215 - 460ms/epoch - 17ms/step\n",
      "Epoch 13/50\n",
      "27/27 - 0s - loss: 0.0176 - accuracy: 0.9581 - c_precision: 0.9903 - c_recall: 0.9242 - auc: 0.9990 - c_fbeta: 0.9265 - c_f1_macro: nan - hammingloss: 0.0055 - val_loss: 0.2695 - val_accuracy: 0.8387 - val_c_precision: 0.8292 - val_c_recall: 0.7935 - val_auc: 0.9590 - val_c_fbeta: 0.7935 - val_c_f1_macro: nan - val_hammingloss: 0.0228 - 457ms/epoch - 17ms/step\n",
      "Epoch 14/50\n",
      "27/27 - 0s - loss: 0.0151 - accuracy: 0.9653 - c_precision: 0.9881 - c_recall: 0.9377 - auc: 0.9993 - c_fbeta: 0.9393 - c_f1_macro: nan - hammingloss: 0.0048 - val_loss: 0.2374 - val_accuracy: 0.8387 - val_c_precision: 0.8391 - val_c_recall: 0.7935 - val_auc: 0.9547 - val_c_fbeta: 0.7935 - val_c_f1_macro: nan - val_hammingloss: 0.0222 - 455ms/epoch - 17ms/step\n",
      "Epoch 15/50\n",
      "27/27 - 0s - loss: 0.0125 - accuracy: 0.9820 - c_precision: 0.9930 - c_recall: 0.9529 - auc: 0.9995 - c_fbeta: 0.9533 - c_f1_macro: nan - hammingloss: 0.0035 - val_loss: 0.2134 - val_accuracy: 0.8280 - val_c_precision: 0.8391 - val_c_recall: 0.7935 - val_auc: 0.9568 - val_c_fbeta: 0.7935 - val_c_f1_macro: nan - val_hammingloss: 0.0222 - 460ms/epoch - 17ms/step\n",
      "Epoch 16/50\n",
      "27/27 - 0s - loss: 0.0097 - accuracy: 0.9856 - c_precision: 0.9939 - c_recall: 0.9597 - auc: 0.9998 - c_fbeta: 0.9607 - c_f1_macro: nan - hammingloss: 0.0030 - val_loss: 0.1834 - val_accuracy: 0.8387 - val_c_precision: 0.8391 - val_c_recall: 0.7935 - val_auc: 0.9529 - val_c_fbeta: 0.7935 - val_c_f1_macro: nan - val_hammingloss: 0.0222 - 452ms/epoch - 17ms/step\n",
      "Epoch 17/50\n",
      "27/27 - 0s - loss: 0.0088 - accuracy: 0.9820 - c_precision: 0.9894 - c_recall: 0.9656 - auc: 0.9999 - c_fbeta: 0.9662 - c_f1_macro: nan - hammingloss: 0.0029 - val_loss: 0.1605 - val_accuracy: 0.8280 - val_c_precision: 0.8391 - val_c_recall: 0.7935 - val_auc: 0.9538 - val_c_fbeta: 0.7935 - val_c_f1_macro: nan - val_hammingloss: 0.0222 - 473ms/epoch - 18ms/step\n",
      "Epoch 18/50\n",
      "27/27 - 0s - loss: 0.0074 - accuracy: 0.9916 - c_precision: 0.9931 - c_recall: 0.9782 - auc: 0.9999 - c_fbeta: 0.9785 - c_f1_macro: nan - hammingloss: 0.0019 - val_loss: 0.1379 - val_accuracy: 0.8280 - val_c_precision: 0.8391 - val_c_recall: 0.7935 - val_auc: 0.9516 - val_c_fbeta: 0.7935 - val_c_f1_macro: nan - val_hammingloss: 0.0222 - 477ms/epoch - 18ms/step\n",
      "Epoch 19/50\n",
      "27/27 - 0s - loss: 0.0058 - accuracy: 0.9928 - c_precision: 0.9977 - c_recall: 0.9705 - auc: 0.9999 - c_fbeta: 0.9727 - c_f1_macro: nan - hammingloss: 0.0013 - val_loss: 0.1271 - val_accuracy: 0.8387 - val_c_precision: 0.8366 - val_c_recall: 0.7820 - val_auc: 0.9531 - val_c_fbeta: 0.7820 - val_c_f1_macro: nan - val_hammingloss: 0.0228 - 458ms/epoch - 17ms/step\n",
      "Epoch 20/50\n",
      "27/27 - 0s - loss: 0.0058 - accuracy: 0.9940 - c_precision: 0.9910 - c_recall: 0.9931 - auc: 0.9993 - c_fbeta: 0.9921 - c_f1_macro: nan - hammingloss: 0.0010 - val_loss: 0.1070 - val_accuracy: 0.8387 - val_c_precision: 0.8263 - val_c_recall: 0.7913 - val_auc: 0.9526 - val_c_fbeta: 0.7913 - val_c_f1_macro: nan - val_hammingloss: 0.0228 - 452ms/epoch - 17ms/step\n",
      "Epoch 21/50\n",
      "27/27 - 0s - loss: 0.0050 - accuracy: 0.9952 - c_precision: 0.9954 - c_recall: 0.9839 - auc: 1.0000 - c_fbeta: 0.9843 - c_f1_macro: nan - hammingloss: 0.0013 - val_loss: 0.1022 - val_accuracy: 0.8387 - val_c_precision: 0.8373 - val_c_recall: 0.8373 - val_auc: 0.9547 - val_c_fbeta: 0.8354 - val_c_f1_macro: nan - val_hammingloss: 0.0202 - 457ms/epoch - 17ms/step\n",
      "Epoch 22/50\n",
      "27/27 - 0s - loss: 0.0039 - accuracy: 0.9952 - c_precision: 0.9943 - c_recall: 0.9908 - auc: 1.0000 - c_fbeta: 0.9908 - c_f1_macro: nan - hammingloss: 9.7305e-04 - val_loss: 0.1013 - val_accuracy: 0.8387 - val_c_precision: 0.8384 - val_c_recall: 0.7830 - val_auc: 0.9448 - val_c_fbeta: 0.7830 - val_c_f1_macro: nan - val_hammingloss: 0.0228 - 464ms/epoch - 17ms/step\n",
      "Epoch 23/50\n",
      "27/27 - 0s - loss: 0.0038 - accuracy: 0.9976 - c_precision: 0.9965 - c_recall: 0.9861 - auc: 1.0000 - c_fbeta: 0.9864 - c_f1_macro: nan - hammingloss: 0.0011 - val_loss: 0.1025 - val_accuracy: 0.8387 - val_c_precision: 0.8384 - val_c_recall: 0.7830 - val_auc: 0.9395 - val_c_fbeta: 0.7830 - val_c_f1_macro: nan - val_hammingloss: 0.0228 - 459ms/epoch - 17ms/step\n",
      "Epoch 24/50\n",
      "27/27 - 0s - loss: 0.0028 - accuracy: 0.9976 - c_precision: 0.9977 - c_recall: 0.9954 - auc: 0.9999 - c_fbeta: 0.9958 - c_f1_macro: nan - hammingloss: 4.4910e-04 - val_loss: 0.0992 - val_accuracy: 0.8495 - val_c_precision: 0.8331 - val_c_recall: 0.7601 - val_auc: 0.9349 - val_c_fbeta: 0.7601 - val_c_f1_macro: nan - val_hammingloss: 0.0242 - 458ms/epoch - 17ms/step\n",
      "Epoch 25/50\n",
      "27/27 - 0s - loss: 0.0028 - accuracy: 0.9976 - c_precision: 0.9944 - c_recall: 0.9954 - auc: 1.0000 - c_fbeta: 0.9946 - c_f1_macro: nan - hammingloss: 6.7365e-04 - val_loss: 0.0991 - val_accuracy: 0.8387 - val_c_precision: 0.8287 - val_c_recall: 0.7924 - val_auc: 0.9373 - val_c_fbeta: 0.7924 - val_c_f1_macro: nan - val_hammingloss: 0.0228 - 460ms/epoch - 17ms/step\n",
      "Epoch 26/50\n",
      "27/27 - 0s - loss: 0.0029 - accuracy: 0.9952 - c_precision: 0.9954 - c_recall: 0.9942 - auc: 1.0000 - c_fbeta: 0.9938 - c_f1_macro: nan - hammingloss: 6.7365e-04 - val_loss: 0.0986 - val_accuracy: 0.8280 - val_c_precision: 0.8358 - val_c_recall: 0.7716 - val_auc: 0.9455 - val_c_fbeta: 0.7716 - val_c_f1_macro: nan - val_hammingloss: 0.0235 - 475ms/epoch - 18ms/step\n",
      "Epoch 27/50\n",
      "27/27 - 0s - loss: 0.0024 - accuracy: 0.9976 - c_precision: 0.9966 - c_recall: 0.9931 - auc: 1.0000 - c_fbeta: 0.9927 - c_f1_macro: nan - hammingloss: 6.7365e-04 - val_loss: 0.0989 - val_accuracy: 0.8280 - val_c_precision: 0.8415 - val_c_recall: 0.8039 - val_auc: 0.9409 - val_c_fbeta: 0.8039 - val_c_f1_macro: nan - val_hammingloss: 0.0215 - 460ms/epoch - 17ms/step\n",
      "Epoch 28/50\n",
      "27/27 - 0s - loss: 0.0025 - accuracy: 0.9976 - c_precision: 0.9977 - c_recall: 0.9807 - auc: 1.0000 - c_fbeta: 0.9805 - c_f1_macro: nan - hammingloss: 6.7365e-04 - val_loss: 0.1009 - val_accuracy: 0.8387 - val_c_precision: 0.8415 - val_c_recall: 0.8039 - val_auc: 0.9291 - val_c_fbeta: 0.8039 - val_c_f1_macro: nan - val_hammingloss: 0.0215 - 452ms/epoch - 17ms/step\n",
      "Epoch 29/50\n",
      "27/27 - 0s - loss: 0.0039 - accuracy: 0.9976 - c_precision: 0.9934 - c_recall: 0.9954 - auc: 1.0000 - c_fbeta: 0.9951 - c_f1_macro: nan - hammingloss: 7.4850e-04 - val_loss: 0.0988 - val_accuracy: 0.8387 - val_c_precision: 0.8415 - val_c_recall: 0.8039 - val_auc: 0.9477 - val_c_fbeta: 0.8039 - val_c_f1_macro: nan - val_hammingloss: 0.0215 - 451ms/epoch - 17ms/step\n",
      "Epoch 30/50\n",
      "27/27 - 0s - loss: 0.0030 - accuracy: 0.9964 - c_precision: 0.9977 - c_recall: 0.9942 - auc: 0.9993 - c_fbeta: 0.9942 - c_f1_macro: nan - hammingloss: 5.2395e-04 - val_loss: 0.0992 - val_accuracy: 0.8280 - val_c_precision: 0.8287 - val_c_recall: 0.7924 - val_auc: 0.9457 - val_c_fbeta: 0.7924 - val_c_f1_macro: nan - val_hammingloss: 0.0228 - 454ms/epoch - 17ms/step\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0973 - accuracy: 0.8017 - c_precision: 0.8105 - c_recall: 0.7227 - auc: 0.9420 - c_fbeta: 0.7181 - c_f1_macro: nan - hammingloss: 0.0256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2176299/945050786.py:159: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_evaluate = df_evaluate.append(df_row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "RNNModel(btype_labels_map,dev_labels_map,btype_y_train,dev_y_train,btype_y_test,dev_y_test,5,True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d93f6640fd87e5f8c6032ac735eb7d0e0d3eac2795728759269418eb5c1b2092"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
